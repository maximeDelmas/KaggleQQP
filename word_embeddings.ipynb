{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import pos_tag, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque sentense de la question-pairs:\n",
    "\n",
    "1) Tokenization en utilisant *word_tokenize* de nltk. C'est le tokenizer par défaut de la librairie nltk.tokenize\n",
    "\n",
    "2) to.lower ()\n",
    "\n",
    "3) POS tagging: On souhaite faire de la lematization, plutôt que juste faire du steming. Cependant pour pouvoir appliquer les algorithmes de lematization, on a besoin de savoir si le mot est employé comme verbe, nom ou ajectif etc. dans la phrase. Il faut donc faire un pré-traitement de POS-tagging\n",
    "\n",
    "4) Avant d'appliquer l'algo de lemmatisation, on va remove les stop words. On a utiliser une liste personnalité de stop words. J'ai notamment rajouter \"need\", \"should\", \"would\", \"n't\". L'absence de \"need\", \"should\", \"would\" est étrange mais j'avais lu dans un article que justement beaucoup de limites avaient été identifiés par rapport aux liste de stop-words et que certaines sont plus ou moins complète. Pour \"n't\" je l'ai rajouté à cause du processus de tokenization que l'on utilise avec *word_tokenize* car il va par exemple tokenizer \"wouldn't\" en \"would\" + \"n't\".\n",
    "\n",
    "5) On map le tag de treebank sur les tag de wordnet. En fait le process de pos-tagging que l'on utilise avec ntlk nous tag les tokens en utilisant les tags de tree-bank, or, ensuite on utilise le lemmatizer de wordnet et donc il nous faut les tags de wordNet qui sont restreint à Adjectif, nom, verbe et adverbe. On réalise donc un mapping\n",
    "\n",
    "6) Enfin on applique le lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentense(sentense, stop_words, lemmatizer):\n",
    "    \"\"\"tokenize an input sentense\n",
    "\n",
    "    Args:\n",
    "        sentense (str): the input sentense\n",
    "        stop_words (list): the stop word list\n",
    "        lemmatizer (WordNetLemmatizer): the WordNet lemmatizer\n",
    "\n",
    "    Returns:\n",
    "        tokens: the token vector list\n",
    "    \"\"\"\n",
    "\n",
    "    def get_wordnet_tag(treebank_tag):\n",
    "\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    stop_words_and_punct = stop_words + list(string.punctuation) \n",
    "    \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(sentense)\n",
    "    \n",
    "    # to lower\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # postag \n",
    "    tokens = pos_tag(tokens)\n",
    "    \n",
    "    # remove stop words from stop_words list\n",
    "    tokens = [(tks, tag) for (tks, tag) in tokens if tks not in stop_words_and_punct]\n",
    "\n",
    "    # Transform tree_bank_tag to wordnet tag\n",
    "    tokens = [(tks, get_wordnet_tag(tag)) for (tks, tag) in tokens]\n",
    "\n",
    "    # lemmatization with pos-tagging\n",
    "    tokens = [lemmatizer.lemmatize(tks, tag) if tag else tks for (tks, tag) in tokens]\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def prepare_data_and_vocabulary(data, path_stop_words):\n",
    "    \"\"\"Prepare the dataset\n",
    "\n",
    "    Args:\n",
    "        data (pd.Dataframe): the input panda dataframe\n",
    "        path_stop_words (str): path to the stop word file \n",
    "\n",
    "    Returns:\n",
    "        dataset (list): [token_list_q1, token_list_q1]\n",
    "        voc (dict): the vocabulary dict\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read stop words\n",
    "    stop_words = []\n",
    "    with open(path_stop_words, \"r\") as stop_words_f:\n",
    "        for w in stop_words_f:\n",
    "            stop_words.append(w.rstrip())\n",
    "    \n",
    "    # Init nltk stemmer\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    # Initialyze vocabulary\n",
    "    voc = defaultdict(int)\n",
    "\n",
    "    # Initialyze dataset\n",
    "    dataset = list()\n",
    "    \n",
    "    # Interate over sentenses\n",
    "    for index, row in data.iterrows():\n",
    "        tokens_s1 = tokenize_sentense(row[\"question1\"], stop_words, lem)\n",
    "        tokens_s2 = tokenize_sentense(row[\"question2\"], stop_words, lem)\n",
    "\n",
    "        dataset.append((tokens_s1, tokens_s2))\n",
    "        \n",
    "        # Increment voc\n",
    "        for t in tokens_s1 + tokens_s2:\n",
    "            voc[t] += 1\n",
    "    \n",
    "    return dataset, voc\n",
    "\n",
    "def prepare_data_and_vocabulary_2(data):\n",
    "    \"\"\"Prepare the dataset nbut without stop words filtering or lemming\n",
    "\n",
    "    Args:\n",
    "        data (pd.Dataframe): the input panda dataframe\n",
    "\n",
    "    Returns:\n",
    "        dataset (list): [token_list_q1, token_list_q1]\n",
    "        voc (dict): the vocabulary dict\n",
    "    \"\"\"\n",
    "    # Initialyze vocabulary\n",
    "    voc = defaultdict(int)\n",
    "\n",
    "    # Initialyze dataset\n",
    "    dataset = list()\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "    # Interate over sentenses\n",
    "    for index, row in data.iterrows():\n",
    "        tokens_s1 = tokenizer.tokenize(row[\"question1\"].lower())\n",
    "        tokens_s2 = tokenizer.tokenize(row[\"question2\"].lower())\n",
    "\n",
    "        dataset.append((tokens_s1, tokens_s2))\n",
    "        \n",
    "        # Increment voc\n",
    "        for t in tokens_s1 + tokens_s2:\n",
    "            voc[t] += 1\n",
    "    \n",
    "    return dataset, voc\n",
    "\n",
    "\n",
    "def create_sentense_vectors(tokens, embedding, d):\n",
    "    \n",
    "    l = len(tokens)\n",
    "    e = np.empty((l, d))\n",
    "    \n",
    "    for i in range(l):\n",
    "        if tokens[i] in embedding.keys():\n",
    "            e[i, :] = embedding[tokens[i]]\n",
    "        \n",
    "        else:\n",
    "            e[i, :] = embedding[\"[UNK]\"]\n",
    "    \n",
    "    return e\n",
    "\n",
    "def create_dataset_vectors(dataset, embedding, d):\n",
    "    \n",
    "    vectorized_dataset = []\n",
    "\n",
    "    for question_pair in dataset:\n",
    "        vectorized_dataset.append((create_sentense_vectors(question_pair[0], embedding, d), create_sentense_vectors(question_pair[1], embedding, d)))\n",
    "    \n",
    "    return vectorized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For test Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"data/test/test_set.csv\", index_col = False)\n",
    "\n",
    "y = data[\"is_duplicate\"].tolist()\n",
    "\n",
    "\n",
    "# Create voc and tokenized data\n",
    "# WITH STOP WORDS AND LEMMING\n",
    "# dataset, voc = prepare_data_and_vocabulary(data, \"data/utils/stop_words.txt\")\n",
    "\n",
    "# WITHOUT STOP WORDS AND LEMMING\n",
    "dataset, voc = prepare_data_and_vocabulary_2(data)\n",
    "\n",
    "# Check empty vectors\n",
    "empty_q  = [(len(i) == 0 or len(j) == 0) for i, j in dataset]\n",
    "print(\"There are \" + str(sum(empty_q)) + \" pair of sentences when at least one has an empty tokenized representation. They will be removed.\")\n",
    "for i in range(len(dataset)):\n",
    "    if empty_q[i]:\n",
    "        print(dataset[i])\n",
    "        print(data.loc[i])\n",
    "\n",
    "dataset = [dataset[i] for i in range(len(dataset)) if not empty_q[i]]\n",
    "y = [y[i] for i in range(len(y)) if not empty_q[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(file, d):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    \n",
    "    # For UNK token references: https://github.com/keras-team/keras/issues/12124 \n",
    "    unk = np.zeros(d)\n",
    "\n",
    "    with open(file,'r') as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "            unk += embedding\n",
    "    \n",
    "    # Add [UNK] token random vector\n",
    "    n = len(glove_model)\n",
    "    unk = unk/n\n",
    "    glove_model[\"[UNK]\"] = unk\n",
    "\n",
    "    print(f\"{n + 1} words loaded!\")\n",
    "\n",
    "    return glove_model\n",
    "\n",
    "# Get Glove Embedding\n",
    "d = 300\n",
    "glove = load_glove_model(\"data/utils/glove.6B/glove.6B.300d.txt\", d=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def create_w2v_subset_embedding(word2vec, voc):\n",
    "    \n",
    "    word_embedding = dict()\n",
    "    word_embedding[\"[UNK]\"] = word2vec.vectors.mean(axis=0)\n",
    "\n",
    "    available_words = word2vec.key_to_index.keys()\n",
    "\n",
    "    for word in voc:\n",
    "        if word in available_words:\n",
    "            word_embedding[word] = word2vec[word]\n",
    "    \n",
    "    return word_embedding\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('data/utils/Word2Vec/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "subset_word2vec = create_w2v_subset_embedding(w2v, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_glove = create_dataset_vectors(dataset, glove, d=d)\n",
    "\n",
    "dataset_vord2vec = create_dataset_vectors(dataset, subset_word2vec, d=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling and Pairwise distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pooling(tokens_vectors):\n",
    "    return tokens_vectors.mean(axis=0)\n",
    "\n",
    "def pairwise_distance(dataset, pooling_fn):\n",
    "    pairwise_distance = []\n",
    "\n",
    "    for q_pair in dataset:\n",
    "        pairwise_distance.append(np.linalg.norm(pooling_fn(q_pair[0]) - pooling_fn(q_pair[1])))\n",
    "    \n",
    "    return pairwise_distance\n",
    "\n",
    "glove_pairwise_dist = pairwise_distance(dataset_glove, avg_pooling)\n",
    "word2vec_pairwise_dist = pairwise_distance(dataset_vord2vec, avg_pooling)\n",
    "\n",
    "df_glove = pd.DataFrame({\"pwdist\": glove_pairwise_dist, \"y\": y})\n",
    "df_w2c = pd.DataFrame({\"pwdist\": word2vec_pairwise_dist, \"y\": y})\n",
    "\n",
    "df_glove.to_csv(\"data/test/Glove/pwdist_no_stopwordsLemming_CV.csv\", index=False)\n",
    "df_w2c.to_csv(\"data/test/Word2Vec/pwdist_no_stopwordsLemming_CV.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19c1444b3d5567e4cbdf4788d938f6edea2231b6a36cad91fb20378c11783a94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('QQP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
