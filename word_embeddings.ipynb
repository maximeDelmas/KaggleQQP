{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mxdelmas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque sentense de la question-pairs:\n",
    "\n",
    "1) Tokenization en utilisant *word_tokenize* de nltk. C'est le tokenizer par défaut de la librairie nltk.tokenize\n",
    "\n",
    "2) to.lower ()\n",
    "\n",
    "3) POS tagging: On souhaite faire de la lematization, plutôt que juste faire du steming. Cependant pour pouvoir appliquer les algorithmes de lematization, on a besoin de savoir si le mot est employé comme verbe, nom ou ajectif etc. dans la phrase. Il faut donc faire un pré-traitement de POS-tagging\n",
    "\n",
    "4) Avant d'appliquer l'algo de lemmatisation, on va remove les stop words. On a utiliser une liste personnalité de stop words. J'ai notamment rajouter \"need\", \"should\", \"would\", \"n't\". L'absence de \"need\", \"should\", \"would\" est étrange mais j'avais lu dans un article que justement beaucoup de limites avaient été identifiés par rapport aux liste de stop-words et que certaines sont plus ou moins complète. Pour \"n't\" je l'ai rajouté à cause du processus de tokenization que l'on utilise avec *word_tokenize* car il va par exemple tokenizer \"wouldn't\" en \"would\" + \"n't\".\n",
    "\n",
    "5) On map le tag de treebank sur les tag de wordnet. En fait le process de pos-tagging que l'on utilise avec ntlk nous tag les tokens en utilisant les tags de tree-bank, or, ensuite on utilise le lemmatizer de wordnet et donc il nous faut les tags de wordNet qui sont restreint à Adjectif, nom, verbe et adverbe. On réalise donc un mapping\n",
    "\n",
    "6) Enfin on applique le lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1722698511.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_855/1722698511.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    tokens = [t if t not in stop_words_and_punct for t in tokens]\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentense(sentense, stop_words, lemmatizer):\n",
    "    \"\"\"tokenize an input sentense\n",
    "\n",
    "    Args:\n",
    "        sentense (str): the input sentense\n",
    "        stop_words (list): the stop word list\n",
    "        lemmatizer (WordNetLemmatizer): the WordNet lemmatizer\n",
    "\n",
    "    Returns:\n",
    "        tokens: the token vector list\n",
    "    \"\"\"\n",
    "\n",
    "    def get_wordnet_tag(treebank_tag):\n",
    "\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    stop_words_and_punct = stop_words + list(string.punctuation) \n",
    "    \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(sentense)\n",
    "    \n",
    "    # to lower\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # postag \n",
    "    tokens = pos_tag(tokens)\n",
    "    \n",
    "    # remove stop words from stop_words list\n",
    "    tokens = [(tks, tag) for (tks, tag) in tokens if tks not in stop_words_and_punct]\n",
    "\n",
    "    # Transform tree_bank_tag to wordnet tag\n",
    "    tokens = [(tks, get_wordnet_tag(tag)) for (tks, tag) in tokens]\n",
    "\n",
    "    # lemmatization with pos-tagging\n",
    "    tokens = [lemmatizer.lemmatize(tks, tag) if tag else tks for (tks, tag) in tokens]\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "\n",
    "def prepare_data_and_vocabulary(data, path_stop_words):\n",
    "    \"\"\"Prepare the dataset\n",
    "\n",
    "    Args:\n",
    "        data (pd.Dataframe): the input panda dataframe\n",
    "        path_stop_words (str): path to the stop word file \n",
    "\n",
    "    Returns:\n",
    "        dataset (list): [token_list_q1, token_list_q1]\n",
    "        voc (dict): the vocabulary dict\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read stop words\n",
    "    stop_words = []\n",
    "    with open(path_stop_words, \"r\") as stop_words_f:\n",
    "        for w in stop_words_f:\n",
    "            stop_words.append(w.rstrip())\n",
    "    \n",
    "    # Init nltk stemmer\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    # Initialyze vocabulary\n",
    "    voc = defaultdict(int)\n",
    "\n",
    "    # Initialyze dataset\n",
    "    dataset = list()\n",
    "    \n",
    "    # Interate over sentenses\n",
    "    for index, row in data.iterrows():\n",
    "        tokens_s1 = tokenize_sentense(row[\"question1\"], stop_words, lem)\n",
    "        tokens_s2 = tokenize_sentense(row[\"question2\"], stop_words, lem)\n",
    "\n",
    "        dataset.append((tokens_s1, tokens_s2))\n",
    "        \n",
    "        # Increment voc\n",
    "        for t in tokens_s1 + tokens_s2:\n",
    "            voc[t] += 1\n",
    "    \n",
    "    return dataset, voc\n",
    "\n",
    "\n",
    "def load_glove_model(file, d):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    \n",
    "    # For UNK token references: https://github.com/keras-team/keras/issues/12124 \n",
    "    unk = np.zeros(d)\n",
    "\n",
    "    with open(file,'r') as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "            unk += embedding\n",
    "    \n",
    "    # Add [UNK] token random vector\n",
    "    n = len(glove_model)\n",
    "    unk = unk/n\n",
    "    glove_model[\"[UNK]\"] = unk\n",
    "\n",
    "    print(f\"{n + 1} words loaded!\")\n",
    "\n",
    "    return glove_model\n",
    "\n",
    "def create_sentense_vectors(tokens, embedding, d):\n",
    "    \n",
    "    l = len(tokens)\n",
    "    e = np.empty((l, d))\n",
    "    \n",
    "    for i in range(l):\n",
    "        if tokens[i] in embedding.keys():\n",
    "            e[i, :] = embedding[tokens[i]]\n",
    "        \n",
    "        else:\n",
    "            e[i, :] = embedding[\"[UNK]\"]\n",
    "    \n",
    "    return e\n",
    "\n",
    "def create_dataset_vectors(dataset, embedding, d):\n",
    "    \n",
    "    vectorized_dataset = []\n",
    "\n",
    "    for question_pair in dataset:\n",
    "        vectorized_dataset.append((create_sentense_vectors(question_pair[0], embedding, d), create_sentense_vectors(question_pair[1], embedding, d)))\n",
    "    \n",
    "    return vectorized_dataset\n",
    "\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv(\"data/CV/cv_data.csv\", index_col = False)\n",
    "\n",
    "# Create voc and tokenized data\n",
    "dataset, voc = prepare_data_and_vocabulary(data, \"data/utils/stop_words.txt\")\n",
    "\n",
    "# Get Glove Embedding\n",
    "glove = load_glove_model(\"data/utils/glove.6B/glove.6B.50d.txt\", d=50)\n",
    "\n",
    "dataset_glove = create_dataset_vectors(dataset, glove, d=50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19c1444b3d5567e4cbdf4788d938f6edea2231b6a36cad91fb20378c11783a94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('QQP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
