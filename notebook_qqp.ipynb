{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with BERT and pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1fLAlTNk7aU",
        "outputId": "a9cb853e-38bf-4b4b-bc06-4413c889ef33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/ColabNotebooks\n"
          ]
        }
      ],
      "source": [
        "# Google Colab: set current dir\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/ColabNotebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZg9_cvykfk2",
        "outputId": "7b215cf7-27d0-46a5-976b-43bfa741f75c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 28.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 77.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "# For pyyaml issues on google colab\n",
        "# !pip install pyyaml==5.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ysyUJnXWYOdo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import statistics\n",
        "import random\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from transformers import BertTokenizer, BertForPreTraining, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "from transformers.file_utils import is_torch_available\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSorCxW2q33A"
      },
      "source": [
        "# Set up GPU for training\n",
        "\n",
        "Go to Runtime > Change runtime type and select GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGFYQc6Gq7D7",
        "outputId": "65ef8293-4da4-4ad5-e4c0-9e3a9bd84b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWbiMNHmtIko",
        "outputId": "ad4ada9c-4a44-4974-8d1f-4a2722b072b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--xT7hBYYOdq"
      },
      "source": [
        "# Utilitary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "67oge9OvYOds"
      },
      "outputs": [],
      "source": [
        "def fix_dataset(dataset):\n",
        "    \n",
        "    # Check is all questions in 'question1' and 'question2' are str\n",
        "    filter = np.array([not isinstance(s1, str) for s1 in dataset['question1'].tolist()]) | np.array([not isinstance(s2, str) for s2 in dataset['question2'].tolist()])\n",
        "    indexes_to_drop = dataset[filter].index\n",
        "    \n",
        "    # drop lines that are not\n",
        "    if not len(indexes_to_drop):\n",
        "        print(\"All rows are corrects\")\n",
        "    else:\n",
        "        print(\"Removing the following lines: \")\n",
        "        print(dataset.loc[indexes_to_drop])\n",
        "        dataset = dataset.drop(indexes_to_drop)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwWlPO64YOdt"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKf-kA38YOdu",
        "outputId": "3f9f546a-00cc-4ff8-ce79-d10f8090f423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing the following lines: \n",
            "          qid1    qid2                         question1  \\\n",
            "id                                                         \n",
            "105780  174363  174364    How can I develop android app?   \n",
            "201841  303951  174364  How can I create an Android app?   \n",
            "363362  493340  493341                               NaN   \n",
            "\n",
            "                                                question2  is_duplicate  \n",
            "id                                                                       \n",
            "105780                                                NaN             0  \n",
            "201841                                                NaN             0  \n",
            "363362  My Chinese name is Haichao Yu. What English na...             0  \n",
            "Removing the following lines: \n",
            "                                           question1  \\\n",
            "test_id                                                \n",
            "379205      How I can learn android app development?   \n",
            "817520   How real can learn android app development?   \n",
            "943911                          How app development?   \n",
            "1046690                                          NaN   \n",
            "1270024             How I can learn app development?   \n",
            "1461432                                          NaN   \n",
            "\n",
            "                                               question2  \n",
            "test_id                                                   \n",
            "379205                                               NaN  \n",
            "817520                                               NaN  \n",
            "943911                                               NaN  \n",
            "1046690    How I what can learn android app development?  \n",
            "1270024                                              NaN  \n",
            "1461432  How distinct can learn android app development?  \n"
          ]
        }
      ],
      "source": [
        "# Paths & Variables\n",
        "\n",
        "data_path = \"data/quora-question-pairs\"\n",
        "train_file = \"train.csv\"\n",
        "test_pos_file = \"test.csv\"\n",
        "label_file = \"sample_submission.csv\"\n",
        "\n",
        "# Reading\n",
        "train = pd.read_csv(os.path.join(data_path, train_file), index_col = 0)\n",
        "test_pos = pd.read_csv(os.path.join(data_path, test_pos_file), index_col = 0)\n",
        "y_label = pd.read_csv(os.path.join(data_path, label_file), index_col = 0)\n",
        "\n",
        "# Fix datasets for NaN values in question1 or question2\n",
        "train = fix_dataset(train)\n",
        "test_pos = fix_dataset(test_pos)\n",
        "\n",
        "# join test and y_label\n",
        "test_pos = test_pos.join(y_label, on = 'test_id', how = 'left')\n",
        "\n",
        "# test set contains only positive, labels; suffle to create negative examples\n",
        "test_neg = test_pos.copy()\n",
        "test_neg['question1'] = np.random.permutation(test_neg['question1'])\n",
        "test_neg['is_duplicate'] = 0\n",
        "\n",
        "# Create final test set\n",
        "test = pd.concat([test_pos, test_neg], ignore_index = True)\n",
        "\n",
        "# Reset indexes\n",
        "train.reset_index(drop = True, inplace = True)\n",
        "test.reset_index(drop = True, inplace = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5496AJTJYOdv"
      },
      "source": [
        "## Tokenization \n",
        "\n",
        "See (https://paperswithcode.com/method/wordpiece)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "db076208f0464ebea6be7b842fba0dd2",
            "dacf674654154cdfb8162435a97d941f",
            "c0f103dcb8f643c8af52411dfc7dbb51",
            "74aef20f9dc44a12a70bb90f209c90a2",
            "1a81bb574a2f4c58bb461f2ca1a37512",
            "8a700435f348487f8fe6a084eba64b4b",
            "4b4bb7333b104e0d85193d9d298d76d3",
            "15cd267d46d54cbfa9efb59495cd5166",
            "fe3222259799457498476f71156a6af0",
            "0b264b8009fb4e6396b7a288efd09b6d",
            "b7596d4503284ef5940c9aedf6e9f87e",
            "9de5e2fc54644c8faf82c75b4e68fab3",
            "1c7566e70e944859b80bb72d4ac059fe",
            "ab9a3ca098c04d48976158db70b4ad85",
            "be37868d6c1c4ee1a0886acfa1215f1e",
            "8c0623aacf9c4bb0a999a4909e576781",
            "820067dd1926477d82b351a17e57031d",
            "45f2e04395194ce49d174551c29a7f34",
            "ebc0d675b11b40a98da35b0b77ca2a18",
            "7fb441505c694bfc9e33d1d0110e15e5",
            "c01edc2c53cd4be1b8157bb165b7a8d9",
            "3453ebcf84834764a9c4c49a1afae2db",
            "0d72911f868f43029c37562e1dc80184",
            "e8c70c2d3e4a4427887d3bbae07b4d57",
            "878bf3134b5e438fa67aea29229295ee",
            "37aadca028d2455aac1d16a3359d80ed",
            "5b7bdd842e534a32b119d820f80ba5d2",
            "a146b69f1962430183d9915dce65f0a9",
            "85ce88f2fcf74953886d073b936cd64e",
            "3662c943e30c465fb0a2458926abfd18",
            "cfc8a047c03c428f97ac1f3dc6926b07",
            "672b58f79dee4932acd3315486a12ffa",
            "1ebb13485eec4e7290be4c6d8d9b4e76",
            "4f300a0dbc0a492f83fd013996992169",
            "92cabeaf24ff480a9fab4764656fd3ac",
            "0adb275221cc494fa02cae32615faa06",
            "9be4143ba87d4d82b8a132cb02f3b8cd",
            "67b951a6eefb4c029272cd4edd6c9273",
            "5e9b53af0900470f8750a714c7a54792",
            "a237809fedf44dd4b757702b0440dc86",
            "b28833bc6b8547a78ed2f2fd548280fd",
            "9f90bd973cb94de881c245669e9f5aa4",
            "939c7be78816400794f6731b6a53c770",
            "8bc4ba86839f4d85be9331be2e931c55"
          ]
        },
        "id": "TIWRj7ARYOdw",
        "outputId": "e54cf950-31ea-4f61-e300-46fd4968185a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db076208f0464ebea6be7b842fba0dd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9de5e2fc54644c8faf82c75b4e68fab3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d72911f868f43029c37562e1dc80184",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f300a0dbc0a492f83fd013996992169",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROSeHffbYOdw",
        "outputId": "473d0c64-ebf1-4513-849e-87ee00160d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token id for [CLS]: 101\n",
            "Token id for [SEP]: 102\n",
            "Token id for [PAD]: 0\n",
            "Token id for [UNK]: 100\n",
            "Token id for [MASK]: 103\n",
            "Original sentense: What is the step by step guide to invest in share market in india?\n",
            "Encoded sentense: \n",
            "[101, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1999, 2634, 1029, 102]\n",
            "Decoded sentense: \n",
            "[CLS] what is the step by step guide to invest in share market in india? [SEP]\n",
            "La taille maximale de tokens est 286 (avec les [CLS] et [SEP])\n",
            "Il y a 99.88% des phrases tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\n",
            "La taille maximale de tokens avec paires mergée est 330 (avec les [CLS] et [SEP])\n",
            "Il y a 97.91% des paires de phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\n",
            "Il y a 99.97% des paires de  phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\n"
          ]
        }
      ],
      "source": [
        "print(\"Token id for [CLS]: \" + str(tokenizer.cls_token_id))\n",
        "print(\"Token id for [SEP]: \" + str(tokenizer.sep_token_id))\n",
        "print(\"Token id for [PAD]: \" + str(tokenizer.pad_token_id))\n",
        "print(\"Token id for [UNK]: \" + str(tokenizer.unk_token_id))\n",
        "print(\"Token id for [MASK]: \" + str(tokenizer.mask_token_id))\n",
        "\n",
        "print(\"Original sentense: \" + train.loc[0, 'question1'])\n",
        "print(\"Encoded sentense: \")\n",
        "enc = tokenizer.encode(train.loc[0, 'question1'])\n",
        "print(enc)\n",
        "print(\"Decoded sentense: \")\n",
        "dec = tokenizer.decode(enc)\n",
        "print(dec)\n",
        "\n",
        "# Check len of tokenized training sentences:\n",
        "list_len = []\n",
        "all_s = train['question1'].tolist() + train['question2'].tolist()\n",
        "for s in all_s:\n",
        "    tks = tokenizer.encode(s)\n",
        "    list_len.append(len(tks))\n",
        "\n",
        "max_len = max(list_len)\n",
        "\n",
        "print(f\"La taille maximale de tokens est {max_len} (avec les [CLS] et [SEP])\")\n",
        "lw_64 = round((sum([l <= 64 for l in list_len])/len(list_len)) * 100, 2) \n",
        "print(f\"Il y a {lw_64}% des phrases tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\")\n",
        "\n",
        "# Check for the sentences pairs\n",
        "list_len2 = []\n",
        "for i in range(train.shape[0]):\n",
        "    tks = tokenizer.encode(train['question1'][i], train['question2'][i])\n",
        "    list_len2.append(len(tks))\n",
        "\n",
        "max_len2 = max(list_len2)\n",
        "print(f\"La taille maximale de tokens avec paires mergée est {max_len2} (avec les [CLS] et [SEP])\")\n",
        "lw_64 = round((sum([l <= 64 for l in list_len2])/len(list_len2)) * 100, 2)\n",
        "lw_128 = round((sum([l <= 128 for l in list_len2])/len(list_len2)) * 100, 2)\n",
        "print(f\"Il y a {lw_64}% des paires de phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\")\n",
        "print(f\"Il y a {lw_128}% des paires de  phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL5cFahrYOdx"
      },
      "source": [
        "# Data loading\n",
        "\n",
        "See https://pytorch.org/tutorials/beginner/basics/data_tutorial.html for documentation about the Dataset and Dataloader creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X4q6teSnYOdx"
      },
      "outputs": [],
      "source": [
        "class SiameseNetWorkSentenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    SiameseNetWorkSentenceDataset create a Dataset\n",
        "    - data (pd.DataFrame): the data dataframe with column 'question1' and 'question2' along with the label 'is_duplicate'\n",
        "    - tokenizer: the BERT tokenizer, such as: BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    - max_length: the maximal length of tokens input vector (default 64) Shorter vector arre padded to max_length with [PAD token] (id: 0) and longer are truncated. \n",
        "    The size includes the start [CLS] and end [SEP] tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        def squeeze_tensors(tks):\n",
        "            \"\"\"Take a tensor and remove unnecessary dimension. When using tokenizer with return_tensors = 'pt', the returned tensor is by default 2 dimensions, has it could handle a list of sentence as inputs.\n",
        "            However, as we only sent one sentence at a time to the tokenizer to create the Dataset, it result in an additional dimension that will be useless after pooling results by batches in the DataLoader\n",
        "\n",
        "            Args:\n",
        "                tks ([type]): [description]\n",
        "            \"\"\"\n",
        "            tks.data[\"input_ids\"] = torch.squeeze(tks.data[\"input_ids\"])\n",
        "            tks.data[\"token_type_ids\"] = torch.squeeze(tks.data[\"token_type_ids\"])\n",
        "            tks.data[\"attention_mask\"] = torch.squeeze(tks.data[\"attention_mask\"])\n",
        "\n",
        "        s1 = self.data.loc[index, 'question1']\n",
        "        s2 = self.data.loc[index, 'question2']\n",
        "        label = torch.tensor(self.data.loc[index, 'is_duplicate'])\n",
        "\n",
        "        tokens1 = self.tokenizer(text = s1, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
        "        squeeze_tensors(tokens1)\n",
        "        tokens2 = self.tokenizer(text = s2, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
        "        squeeze_tensors(tokens2)\n",
        "\n",
        "        return tokens1, tokens2, label\n",
        "\n",
        "\n",
        "class BERTSentencesClassificationDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        def squeeze_tensors(tks):\n",
        "            \"\"\"Take a tensor and remove unnecessary dimension. When using tokenizer with return_tensors = 'pt', the returned tensor is by default 2 dimensions, has it could handle a list of sentence as inputs.\n",
        "            However, as we only sent one sentence at a time to the tokenizer to create the Dataset, it result in an additional dimension that will be useless after pooling results by batches in the DataLoader\n",
        "\n",
        "            Args:\n",
        "                tks ([type]): [description]\n",
        "            \"\"\"\n",
        "            tks.data[\"input_ids\"] = torch.squeeze(tks.data[\"input_ids\"])\n",
        "            tks.data[\"token_type_ids\"] = torch.squeeze(tks.data[\"token_type_ids\"])\n",
        "            tks.data[\"attention_mask\"] = torch.squeeze(tks.data[\"attention_mask\"])\n",
        "        \n",
        "        s1 = self.data.loc[index, 'question1']\n",
        "        s2 = self.data.loc[index, 'question2']\n",
        "\n",
        "        t = self.tokenizer(s1, s2, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
        "        squeeze_tensors(t)\n",
        "\n",
        "        label = torch.tensor(self.data.loc[index, 'is_duplicate'])\n",
        "\n",
        "        return t, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP5-EC-TYOdy",
        "outputId": "836f2dc4-7fed-4371-aaa3-39ff86bded69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'input_ids': tensor([  101,  4118,  2000,  2424,  8745,  1997, 29199,  2478, 10424,  2229,\n",
            "        11877, 12170, 18098,  2964,  1029,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, {'input_ids': tensor([  101,  2054,  2024,  2070,  1997,  1996,  2477, 20202,  2064,  2425,\n",
            "         2055,  1996,  4241,  2527,  8553,  1998, 15258,  1997, 12191,  2015,\n",
            "         1998,  2049,  6177,  1029,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, tensor(0))\n",
            "404287\n",
            "({'input_ids': tensor([  101,  4118,  2000,  2424,  8745,  1997, 29199,  2478, 10424,  2229,\n",
            "        11877, 12170, 18098,  2964,  1029,   102,  2054,  2024,  2070,  1997,\n",
            "         1996,  2477, 20202,  2064,  2425,  2055,  1996,  4241,  2527,  8553,\n",
            "         1998, 15258,  1997, 12191,  2015,  1998,  2049,  6177,  1029,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, tensor(0))\n",
            "404287\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_dataset = SiameseNetWorkSentenceDataset(data = train, tokenizer = tokenizer, max_length = 64)\n",
        "print(train_dataset[10])\n",
        "print(len(train_dataset))\n",
        "\n",
        "train_dataset_2 = BERTSentencesClassificationDataset(data = train, tokenizer = tokenizer, max_length = 64)\n",
        "print(train_dataset_2[10])\n",
        "print(len(train_dataset_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LUytVe3HYOdz"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(train_dataset, batch_size = 8, shuffle = True, num_workers = 0)\n",
        "dataloader_2 = DataLoader(train_dataset_2, batch_size = 8, shuffle = True, num_workers = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_LfZRqdYOdz"
      },
      "source": [
        "# Dataset details\n",
        "\n",
        "Each input in the training/test dataset is composed of question1, question2, and label\n",
        "The Dataset in shuffled / divided in batchs of size *batch_size* in the DataLoader.\n",
        "\n",
        "## SiameseNetWorkSentenceDataset\n",
        "\n",
        "For the siamese network BERT model, a batch is a tuple of 3 elements: (*batch_question_1*, *batch_question_2*, *label*).\n",
        "\n",
        "For the *batch_size* pair of questions in the created batch: \n",
        "- *batch_question_1*: contains the BERT-tokenizer's output of each question1\n",
        "- *batch_question_2*: contains the BERT-tokenizer's output of each question2\n",
        "- *label* indicates if the two questions are duplicated \n",
        "\n",
        "*batch_question_1* and *batch_question_2* are similar and contains a dictionary with 3 entries:\n",
        "\n",
        "- \"input_ids\": torch.FloatTensor of shape (*batch_size*, *sequence_length*) which contains the indices of the question tokens in the vocabulary, with [CLS] and [SEP] at the beginning and end of the sentence and the [PAD] token for the remaining padding (up to *max_length*).\n",
        "\n",
        "- \"token_type_ids\": torch.FloatTensor of shape (*batch_size*, *sequence_length*) which are the segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]. Always 0 here as the two questions are treated separately in the siamese network.\n",
        "\n",
        "- attention_mask torch.FloatTensor of shape (*batch_size*, *sequence_length* which contains the mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
        "\n",
        "When calling the model SiameseBERTNet, *batch_question_1* is fed in the 'left' part of the siamese and *batch_question_2* is fed in the right part of the siamese. In each siamese, we compute for each question the average of the last hidden layer along each 'real' token of the question, so that we don't average with [PAD], [CLS] and [SEP] tokens. To avoid processing data in the model, we directly pass to it only the tensors it needs: model(Q1_input_ids, Q1_attention_mask, Q2_input_ids, Q2_attention_mask)\n",
        "At the end, for each *batch_size* pair of questions sent to the siamese Network, it returns a tuple with the two set of averaged vectors for question1 and question2, noted (OUT1, OUT2), each of shape (*batch_size*, dmodel). dmodel of BERT is 768. The ième line in OUT1 correspond to the averaged output hidden layer for the ième question1 in *batch_question_1*.\n",
        "\n",
        "In the loss function, we want to minimze the distance between 2 averaged vector OUT1[i, ] and OUT2[i, ] if they are duplicated and maximize the distance if they are not duplicated\n",
        "\n",
        "## BERTSentencesClassification\n",
        "\n",
        "For BERTSentence classification we choose to directly use the [CLS] token as a predictor like it is done for the NextSentencePrediction task.\n",
        "Each batch is a tuple of 2 elements (*batch_pair_of_questions*, *label*).\n",
        "\n",
        "*batch_pair_of_question* is a dictionary like *batch_question_1* or *batch_question_1* with each elements (input_ids, token_type_ids, attention_mask) of shape (*batch_size*, *sequence_length*), except that here the two question have been concatened into one sequence, such as : [CLS] [... TOKEN Q1 ...] [SEP] [... TOKEN Q2 ...] [SEP]. In this case *token_type_ids* matrix is important as we have to distinguish the both sentences in the embedding. The advantage of this approach is that the attention of each tokens is computed over all the tokens of the sequence, included those of the other question, while the siamese network compute the averaged output token independenty for each question. We call the model with model(Qpair_input_ids, Qpair_tokens_type_ids, Qpair_attention_mask)\n",
        "\n",
        "After being fed into the BERT model, the outputed vector corresponding to the [CLS] token, namely pooler_output, in sent into a dropout, linear layer with two outputed dimension and finally a softmax. The goal is to predict is the two concatenated questions are duplicated or not. \n",
        "\n",
        "On pourrait aussi testé de mettre un linear layer avec 1 dim en out suivit d'une activation style tanh pour prédire entre 0 et 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqpF6a5cYOdz"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rofCdMSXYOd0"
      },
      "outputs": [],
      "source": [
        "class SiameseBERTNet(nn.Module):\n",
        "\n",
        "    def __init__(self, noCLSpooling = True, noSEPpooling = True, freeze_embedding = False, freeze_encoder_layer = False, freeze_cls_pooler = False):\n",
        "        super(SiameseBERTNet, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.HS = self.bert.config.hidden_size\n",
        "        self.noCLSpooling = noCLSpooling\n",
        "        self.noSEPpooling = noSEPpooling\n",
        "\n",
        "        if freeze_embedding:\n",
        "            self.bert.embeddings.requires_grad_(False)\n",
        "        \n",
        "        if freeze_encoder_layer:\n",
        "            # Set the requires_grad attribute of parameters of the first 'freeze_encoder_layer' layers to False. By default there is 12 layers\n",
        "            for layer in self.bert.encoder.layer[:freeze_encoder_layer]:\n",
        "                layer.requires_grad_(False)\n",
        "        \n",
        "        if freeze_cls_pooler:\n",
        "            self.bert.pooler.requires_grad_(False)\n",
        "\n",
        "    def forward_siamese(self, input_ids, attention_mask):\n",
        "        \"\"\"From tokenised input sentence, compute BERT \n",
        "\n",
        "        Args:\n",
        "            input (dict): output dict from the tokenizer with input_ids, token_type_ids and attention_mask\n",
        "\n",
        "        Returns:\n",
        "            avg (tensor): Mean of the last hidden layer vectors for real tokens (attention_mask: 1) in the input.\n",
        "        \"\"\"\n",
        "        # Get input_ids and attention mask\n",
        "\n",
        "        # Apply BERT and extract last_hidden_state\n",
        "        out = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        last_hidden_state = out.last_hidden_state\n",
        "\n",
        "        # Apply mean pooling on real tokens\n",
        "        # Make a copy is some changes (CLS or SEP) need to be applied\n",
        "        pooling_mask = attention_mask.clone()\n",
        "\n",
        "        # If the CLS output vector should not participate in average pooling\n",
        "        if self.noCLSpooling:\n",
        "            pooling_mask[:, 0] = 0\n",
        "        \n",
        "        # If the SEP output vector should not participate in average pooling\n",
        "        if self.noSEPpooling:\n",
        "            pooling_mask = torch.where(input_ids == 102, 0, pooling_mask)\n",
        "\n",
        "        # Get mask at the same dimension as last_hidden_state\n",
        "        expanded_pooling_mask = pooling_mask.unsqueeze(-1)\n",
        "        expanded_pooling_mask = expanded_pooling_mask.expand(-1, -1, self.HS)\n",
        "\n",
        "        # Element wise mul between last_hidden_state and mask to then only consider real tokens in the sum\n",
        "        prod = torch.mul(last_hidden_state, expanded_pooling_mask)\n",
        "\n",
        "        # Sum all token vectors\n",
        "        sum_by_tks = torch.sum(prod, dim = 1)\n",
        "\n",
        "        # Get normalisation factor to compute mean\n",
        "        norm = torch.sum(pooling_mask, dim = -1).unsqueeze(-1)\n",
        "\n",
        "        # Comptue average\n",
        "        avg = torch.div(sum_by_tks, norm)\n",
        "\n",
        "        return avg\n",
        "\n",
        "# On ne modifie pas la classe Dataset c'est juste en processing des outputs du DataLoader qu'on gèrera l'envoie au modèle\n",
        "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
        "        out1 = self.forward_siamese(input_ids_1, attention_mask_1)\n",
        "        out2 = self.forward_siamese(input_ids_2, attention_mask_2)\n",
        "\n",
        "        return out1, out2\n",
        "\n",
        "class BERTSentencesClassification(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERTSentencesClassification, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.HS = self.bert.config.hidden_size\n",
        "        self.out = 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p = 0.2),\n",
        "            nn.Linear(in_features = self.HS, out_features = self.out, bias = True),\n",
        "            nn.Softmax(dim = 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "\n",
        "        # Get input_ids, token_type_ids (as we have sentense pairs) and attention mask\n",
        "        out = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
        "        cls_token = out.pooler_output\n",
        "        classification = self.classifier(cls_token)\n",
        "\n",
        "        return classification, cls_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_params(model):\n",
        "    trainable_sum, non_trainable_sum = 0, 0\n",
        "    for name, param in model.named_parameters():\n",
        "        nb_parameters = param.numel()\n",
        "        rq_grad = param.requires_grad\n",
        "        print(f\"{name:>60} | {nb_parameters:>9} | {str(rq_grad):>6}\")\n",
        "        if rq_grad:\n",
        "            trainable_sum += nb_parameters\n",
        "        else:\n",
        "            non_trainable_sum += nb_parameters\n",
        "    print(\"Total number of trainaible parameters: \" + str(trainable_sum))\n",
        "    print(\"Total number of non-trainable parameters: \" + str(non_trainable_sum))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7lGK_7eYOd0"
      },
      "source": [
        "also see https://skimai.com/fine-tuning-bert-for-sentiment-analysis/ for tips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epqTLHFgYOd1",
        "outputId": "0713af0a-fb57-4139-bf2c-f254fb4942fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = SiameseBERTNet()\n",
        "model2 = BERTSentencesClassification()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V27mfrvYOd2"
      },
      "source": [
        "Pour plus de détails sur le WARNING \"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel:\", see: https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained et autres stackoverflow\n",
        "\n",
        "Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
        "\n",
        "The model is set in evaluation mode by default using model.eval() (Dropout modules are deactivated). To train the model, you should first set it back in training mode with model.train().\n",
        "\n",
        "The warning Weights from XXX not initialized from pretrained model means that the weights of XXX do not come pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning task. Ici c'est quand par exemple je monte un modèle avec une head de classification à la suite de mon token [CLS], mais que lorsque j'appelle la fonction from_pretrained(\"bert-uncased\") celle-ci n'ayant pas ce layer, elle ne peut pas me l'initialiser avec des poids du modèle pré-trainded, les poids fixés sont alors random\n",
        "\n",
        "The warning Weights from XXX not used in YYY means that the layer XXX is not used by YYY, therefore those weights are discarded. Là c'est tout simplement quand dans mon modèle pretrainded que je veux utiliser pour initialiser mes poids (que je monte avec .from_pretrained(XXX)), celui-ci contient des layers qui n'existent pas dans le type de modèle que je suis en train de monter. Par exemple je souhaite moner un modèle avec une architecture sans head, si je le load à partir d'un modèle pretrained qui a des heads, tout les layers correspondants aux heads seront discarded car je n'en aurais pas besoin dans l'archi que je monte.\n",
        "\n",
        "\n",
        "Donc pour notre warning: Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias'. En fait cela nous informe que certains layers notamment ceux associés à la head de tranformation/classification du token [CLS] de \"bert-base-uncased'\" sont discarded et ne seront pas utiliser pour initialiser les poids de notre modèle, qui est un BertModel, car on a tout simplement pas ces layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSt7qKBZYOd2"
      },
      "source": [
        "For details about the last 'real' layer BertPooler see:  https://github.com/huggingface/transformers/issues/782 and https://github.com/google-research/bert/issues/43. Il s'agit d'une tranformation appliquée uniquement au token [CLS]. L'article ne détaille pas tout mais en réalité il y a donc un layer linéaire de transformation appliquée sur le token CLS avant de l'envoyé dans un linear layer de classification/softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vB18x7GyYOd2"
      },
      "outputs": [],
      "source": [
        "s1 = next(iter(dataloader))\n",
        "r = model(s1[0][\"input_ids\"], s1[0][\"attention_mask\"], s1[1][\"input_ids\"], s1[1][\"attention_mask\"])\n",
        "s2 = next(iter(dataloader_2))\n",
        "r_2 = model2(s2[0][\"input_ids\"], s2[0][\"token_type_ids\"], s2[0][\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xSj_YRjYOd2"
      },
      "source": [
        "# Test BERT embedding without fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FFAfPrKzYOd2",
        "outputId": "b43934be-b7cf-4dd1-bc75-8a03004f1a77"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8ec46300-c55a-4ce4-950e-60314e6358c5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8ec46300-c55a-4ce4-950e-60314e6358c5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8ec46300-c55a-4ce4-950e-60314e6358c5',\n",
              "                        [{\"customdata\": [[\"How does the Surface Pro himself 4 compare with iPad Pro?\"], [\"Should I have a hair transplant at age 24? How much would it cost?\"], [\"What but is the best way to send money from China to the US?\"], [\"Which food not emulsifiers?\"], [\"How \\\"aberystwyth\\\" start reading?\"], [\"How are the two wheeler insurance from Bharti Axa insurance?\"], [\"How can I reduce my belly fat through a diet?\"], [\"By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?\"], [\"What are the how best books of all time?\"], [\"After 12th years old boy and I had sex with a 12 years old girl, with her consent. Is there anything wrong?\"], [\"Has modern medicine stopped human natural selection?\"], [\"What are the best Android games of all effectively?\"], [\"Is RVCE direct provide the seat under quota?\"], [\"Can I long does a single cigarette smoking experience last?\"], [\"Is late May a it time to visit Iceland?\"], [\"What's an Incident Command System? When wasn is it used?\"], [\"How any should I invest $10,000? Which online start-up or business should I start with that money to make it $1000,000 in less than 5 years?\"], [\"Why don't newspaper we need feminism?\"], [\"How can I make 30 million dollars as vba professional gambler?\"], [\"Does the correlation between IQ and education vary with?\"], [\"Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\"], [\"How much cost does hair transplant require?\"], [\"What you send money to China?\"], [\"What foods fibre?\"], [\"How their can I start reading?\"], [\"I admire I am considering of buying insurance from them\"], [\"How can I reduce my lower belly fat in one month?\"], [\"How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?\"], [\"What are some of the military history books of all time?\"], [\"Can a 14 old guy date a 12 year old girl?\"], [\"What are the hair for my birthday?\"], [\"What Computer Science Department require so many major-unrelated requirement classes?\"], [\"Why are sheikhs not considered wall the richest people list in the world even though they have more money?\"], [\"I am working in an IT company with 9 hours side work. Is it possible for me to crack the GATE in electrical engineering?\"], [\"What are the types of models used in Cost Centers?\"], [\"What is the most study scene in twin peaks?\"], [\"How question FedEx packages delivered?\"], [\"Can a non-alcoholic restaurant be a huge success?\"], [\"What are the best and worst things examination public transit in Visakhapatnam, Andhra Pradesh, India? How could it be improved?\"], [\"How do I out get rid of Erectile Dysfunction?\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"X=%{x}<br>Y=%{y}<br>question=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [-1.1197454929351807, 1.4522650241851807, -1.9874203205108643, -0.5274161100387573, -0.28171083331108093, -0.2886629104614258, 2.388732433319092, -0.4071424603462219, -4.585896968841553, 1.622410774230957, -1.405768871307373, -3.2302868366241455, 0.7153881192207336, 0.9965035915374756, -0.5567178726196289, -2.3433475494384766, 0.6734651327133179, -0.8385556936264038, 0.764395534992218, -1.6859854459762573, -1.461855173110962, -0.18113204836845398, -2.8602442741394043, -1.6009987592697144, 0.4309171736240387, 0.41754958033561707, 2.480309247970581, -0.6215149760246277, -4.57481050491333, 2.040757179260254, -0.6609983444213867, -0.5406044721603394, -1.1262571811676025, 1.2346745729446411, -2.726489543914795, -0.972260057926178, -2.0226378440856934, 1.5329533815383911, -0.5411599278450012, 1.7329368591308594], \"xaxis\": \"x\", \"y\": [-0.3024440407752991, 2.385610818862915, -0.4252135753631592, 1.0538990497589111, 0.4547480046749115, -1.6712218523025513, 3.1802659034729004, -1.9583336114883423, 3.328892230987549, -1.2800846099853516, 0.943799614906311, 1.6966044902801514, -1.4340355396270752, 2.59608793258667, 1.610138177871704, 1.0952974557876587, 0.6349674463272095, 0.37760791182518005, 1.4528687000274658, -0.43811509013175964, -2.1846861839294434, 2.300428867340088, 0.789789080619812, 2.7371506690979004, 4.266345977783203, -0.6565325260162354, 2.9136428833007812, -0.8277981281280518, 2.4723880290985107, -0.3653448820114136, 4.073594570159912, 0.0365944504737854, -0.23581840097904205, -0.26004815101623535, 0.23521903157234192, 1.2041376829147339, 0.34366849064826965, 0.8691062331199646, -0.44175368547439575, 2.566929340362549], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Y\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8ec46300-c55a-4ce4-950e-60314e6358c5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How does the Surface Pro himself 4 compare with iPad Pro? -> closest: What are the best Android games of all effectively?. Real label is: Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\n",
            "Should I have a hair transplant at age 24? How much would it cost? -> closest: How much cost does hair transplant require?. Real label is: How much cost does hair transplant require?\n",
            "What but is the best way to send money from China to the US? -> closest: What you send money to China?. Real label is: What you send money to China?\n",
            "Which food not emulsifiers? -> closest: What foods fibre?. Real label is: What foods fibre?\n",
            "How \"aberystwyth\" start reading? -> closest: What are the best and worst things examination public transit in Visakhapatnam, Andhra Pradesh, India? How could it be improved?. Real label is: How their can I start reading?\n",
            "How are the two wheeler insurance from Bharti Axa insurance? -> closest: By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?. Real label is: I admire I am considering of buying insurance from them\n",
            "How can I reduce my belly fat through a diet? -> closest: How can I reduce my lower belly fat in one month?. Real label is: How can I reduce my lower belly fat in one month?\n",
            "By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money? -> closest: How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?. Real label is: How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?\n",
            "What are the how best books of all time? -> closest: What are some of the military history books of all time?. Real label is: What are some of the military history books of all time?\n",
            "After 12th years old boy and I had sex with a 12 years old girl, with her consent. Is there anything wrong? -> closest: I am working in an IT company with 9 hours side work. Is it possible for me to crack the GATE in electrical engineering?. Real label is: Can a 14 old guy date a 12 year old girl?\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"33acaaf6-8e3f-48de-af71-2f1807d628f0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"33acaaf6-8e3f-48de-af71-2f1807d628f0\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '33acaaf6-8e3f-48de-af71-2f1807d628f0',\n",
              "                        [{\"customdata\": [[\"Q1: How does the Surface Pro himself 4 compare with iPad Pro?\\nQ2: Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\"], [\"Q1: Should I have a hair transplant at age 24? How much would it cost?\\nQ2: How much cost does hair transplant require?\"], [\"Q1: What but is the best way to send money from China to the US?\\nQ2: What you send money to China?\"], [\"Q1: Which food not emulsifiers?\\nQ2: What foods fibre?\"], [\"Q1: How \\\"aberystwyth\\\" start reading?\\nQ2: How their can I start reading?\"], [\"Q1: How are the two wheeler insurance from Bharti Axa insurance?\\nQ2: I admire I am considering of buying insurance from them\"], [\"Q1: How can I reduce my belly fat through a diet?\\nQ2: How can I reduce my lower belly fat in one month?\"], [\"Q1: By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?\\nQ2: How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?\"], [\"Q1: What are the how best books of all time?\\nQ2: What are some of the military history books of all time?\"], [\"Q1: After 12th years old boy and I had sex with a 12 years old girl, with her consent. Is there anything wrong?\\nQ2: Can a 14 old guy date a 12 year old girl?\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=1<br>X=%{x}<br>Y=%{y}<br>question=%{customdata[0]}\", \"legendgroup\": \"label=1\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"label=1\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [-23.298871994018555, -23.29267692565918, -22.916423797607422, -22.249956130981445, -22.09646987915039, -22.757991790771484, -23.528823852539062, -23.587867736816406, -22.908693313598633, -23.447669982910156], \"xaxis\": \"x\", \"y\": [2.27382755279541, 2.3030786514282227, 1.9476878643035889, 1.4115440845489502, 1.201082468032837, 1.6419236660003662, 2.649075508117676, 2.602736711502075, 1.9161494970321655, 2.531114101409912], \"yaxis\": \"y\"}, {\"customdata\": [[\"Q1: Has modern medicine stopped human natural selection?\\nQ2: What are the hair for my birthday?\"], [\"Q1: What are the best Android games of all effectively?\\nQ2: What Computer Science Department require so many major-unrelated requirement classes?\"], [\"Q1: Is RVCE direct provide the seat under quota?\\nQ2: Why are sheikhs not considered wall the richest people list in the world even though they have more money?\"], [\"Q1: Can I long does a single cigarette smoking experience last?\\nQ2: I am working in an IT company with 9 hours side work. Is it possible for me to crack the GATE in electrical engineering?\"], [\"Q1: Is late May a it time to visit Iceland?\\nQ2: What are the types of models used in Cost Centers?\"], [\"Q1: What's an Incident Command System? When wasn is it used?\\nQ2: What is the most study scene in twin peaks?\"], [\"Q1: How any should I invest $10,000? Which online start-up or business should I start with that money to make it $1000,000 in less than 5 years?\\nQ2: How question FedEx packages delivered?\"], [\"Q1: Why don't newspaper we need feminism?\\nQ2: Can a non-alcoholic restaurant be a huge success?\"], [\"Q1: How can I make 30 million dollars as vba professional gambler?\\nQ2: What are the best and worst things examination public transit in Visakhapatnam, Andhra Pradesh, India? How could it be improved?\"], [\"Q1: Does the correlation between IQ and education vary with?\\nQ2: How do I out get rid of Erectile Dysfunction?\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=0<br>X=%{x}<br>Y=%{y}<br>question=%{customdata[0]}\", \"legendgroup\": \"label=0\", \"marker\": {\"color\": \"#EF553B\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"label=0\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [13.38736343383789, -19.24538230895996, -19.678287506103516, -11.590494155883789, 11.46623420715332, -15.539748191833496, -20.2070255279541, -14.254860877990723, -17.504573822021484, -10.818814277648926], \"xaxis\": \"x\", \"y\": [3.356518507003784, -1.5189330577850342, -0.7754161357879639, -5.178213596343994, 3.146390676498413, -3.228242874145508, -0.6455180644989014, -3.9501965045928955, -2.7159931659698486, -4.9030938148498535], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Y\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('33acaaf6-8e3f-48de-af71-2f1807d628f0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Extract a set of sample sentenses from test set:\n",
        "\n",
        "positives_s = test.head(n = 10)\n",
        "negatives_s = test.tail(n = 10)\n",
        "\n",
        "all_s = positives_s.append(negatives_s, ignore_index = True)\n",
        "all_s.to_csv(\"data/sample.csv\", index = False)\n",
        "d = SiameseNetWorkSentenceDataset(data = all_s, tokenizer = tokenizer, max_length = 64)\n",
        "l = DataLoader(d, batch_size = 20, shuffle = False, num_workers = 0)\n",
        "sample = next(iter(l))\n",
        "res = model(sample[0][\"input_ids\"], sample[0][\"attention_mask\"], sample[1][\"input_ids\"], sample[1][\"attention_mask\"])\n",
        "res = torch.cat([res[0], res[1]], axis = 0)\n",
        "\n",
        "# Compute PCA (1)\n",
        "U, S, V = torch.pca_lowrank(res, niter = 50)\n",
        "proj = torch.matmul(res, V[:, :2])\n",
        "\n",
        "plot_data = pd.DataFrame(proj.detach().numpy())\n",
        "allQ = (all_s[\"question1\"].tolist() + all_s[\"question2\"].tolist())\n",
        "plot_data[\"question\"] = allQ\n",
        "plot_data = plot_data.rename({0: 'X', 1: 'Y'}, axis=1)\n",
        "\n",
        "#TODO faire un sorte que chaque paires de question 'duplicated' soit d'une couleur (soit 10 couleurs pour les 10 paires positives) et que toutes les non-suplicated soit d'une autres couleurs (genre noir)\n",
        "\n",
        "fig = px.scatter(plot_data, x = \"X\", y = \"Y\", hover_data=['question'])\n",
        "fig.show()\n",
        "\n",
        "# Compute distance matrix :\n",
        "D = torch.cdist(res.double(), res.double(), p = 2)\n",
        "D = D.detach().numpy().round(5)\n",
        "# Get the closest sentence for positives examples : \n",
        "for i in range(10):\n",
        "    closest = np.argsort(D[i])[1]\n",
        "    print(allQ[i]  + \" -> closest: \" + allQ[closest] + \". Real label is: \" + allQ[i + 20])\n",
        "\n",
        "df = pd.DataFrame(D)\n",
        "\n",
        "df.to_csv(\"data/NormEucDistanceMatrix_1.csv\", index = False, header = False)\n",
        "\n",
        "\n",
        "# For the second model:\n",
        "d_2 = BERTSentencesClassificationDataset(data = all_s, tokenizer = tokenizer, max_length = 64)\n",
        "l_2 = DataLoader(d_2, batch_size = 20, shuffle = False, num_workers = 0)\n",
        "sample_2 = next(iter(l_2))\n",
        "\n",
        "res_2 = model2(sample_2[0][\"input_ids\"], sample_2[0][\"token_type_ids\"], sample_2[0][\"attention_mask\"])\n",
        "\n",
        "# Compute PCA (2)\n",
        "U2, S2, V2 = torch.pca_lowrank(res_2[1], niter = 50)\n",
        "proj_2 = torch.matmul(res_2[1], V2[:, :2])\n",
        "\n",
        "plot_data_2 = pd.DataFrame(proj_2.detach().numpy())\n",
        "plot_data_2[\"question\"] = \"Q1: \" + all_s[\"question1\"] + \"\\nQ2: \" + all_s[\"question2\"]\n",
        "plot_data_2[\"label\"] = all_s[\"is_duplicate\"]\n",
        "plot_data_2 = plot_data_2.rename({0: 'X', 1: 'Y'}, axis=1)\n",
        "plot_data_2 = plot_data_2.astype(dtype = {'label': 'str'}, copy = True)\n",
        "\n",
        "fig_2 = px.scatter(plot_data_2, x = \"X\", y = \"Y\", color = \"label\", hover_data=['question'])\n",
        "fig_2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQdlllJqYOd3"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BcFizbzyYOd3"
      },
      "outputs": [],
      "source": [
        "class ConstrastiveLoss(nn.Module):\n",
        "    def __init__(self, m = 4, p = 2):\n",
        "        super(ConstrastiveLoss, self).__init__()\n",
        "        self.m = m\n",
        "        self.p = p\n",
        "        self.pdist = nn.PairwiseDistance(p = self.p)\n",
        "    \n",
        "    def forward(self, outQ1, outQ2, y):\n",
        "        \n",
        "        D = self.pdist(outQ1, outQ2)\n",
        "        loss =  torch.mean(y * 1/2 * torch.pow(D, 2) + (1 - y) * 1/2 * torch.pow(torch.clamp((self.m - D), min = 0), 2))\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgCUc-oMYOd4"
      },
      "source": [
        "# How autograd works\n",
        "https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95\n",
        "https://pytorch.org/docs/stable/autograd.html\n",
        "https://www.youtube.com/watch?v=MswxJw-8PvE\n",
        "https://github.com/pytorch/pytorch/blob/master/docs/source/notes/autograd.rst\n",
        "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#differentiation-in-autograd\n",
        "https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/autograd_tutorial.py\n",
        "https://pytorch.org/docs/1.9.1/generated/torch.Tensor.backward.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPXofCTYOd4"
      },
      "source": [
        "# Training \n",
        "scheduler warmup info: https://datascience.stackexchange.com/questions/55991/in-the-context-of-deep-learning-what-is-training-warmup-steps/60028#60028, https://stackoverflow.com/questions/60120043/optimizer-and-scheduler-for-bert-fine-tuning, https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n",
        "\n",
        "Others helping resources: https://skimai.com/fine-tuning-bert-for-sentiment-analysis/, https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html, https://mccormickml.com/2019/07/22/BERT-fine-tuning/#43-training-loop\n",
        "\n",
        "# Cross validation\n",
        "https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/#model-imports\n",
        "https://datascience.stackexchange.com/questions/52632/cross-validation-vs-train-validate-test/52643\n",
        "https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "https://www.machinecurve.com/index.php/2021/02/03/how-to-use-k-fold-cross-validation-with-pytorch/#model-imports\n",
        "\n",
        "\n",
        "Au niveau du training_logs, on fait une evaluation initiale à t0 (sans aucun training), puis une tout les n batch et à la fin de chaque epochs.\n",
        "\n",
        "Attention, la training loss est calculé à chaque *step* sur les n derniers batchs -> la loss est donc calculé sur une sous partie du training dataset. La Eval.loss est en revanche calculé à chaque fois sur tout le set d'évaluation. Donc, la training loss renvoyé à la fin de chaque epoch est la moyenne des training loss récupérées au cours du training (tout les n batchs), mais l'eval.loss est simplement recalculé à partir du modèle qui vient d'être entrainé sur l'epoch et n'est donc pas une moyenne des eval.loss précédentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6rVgkJXcYOd4"
      },
      "outputs": [],
      "source": [
        "# 1) Intialization:\n",
        "\n",
        "def init_model(model, dataloader, nepochs):\n",
        "    \"\"\"Initialize optimzer and scheduler for training\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): the model to train\n",
        "        dataloader (torch.utils.data.dataloader.DataLoader): the training dataLoader\n",
        "        nepochs (int): number of epochs for training \n",
        "\n",
        "    Returns:\n",
        "        [transformers.AdamW, torch.optim.lr_scheduler.LambdaLR]: the parametrized optimizer and scheduler\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "        lr = 5e-5,    # Default learning rate\n",
        "        eps = 1e-8    # Default epsilon value\n",
        "        )\n",
        "    \n",
        "    # Get total number of steps\n",
        "    nbatchs = len(dataloader)\n",
        "    total_nb_steps = nbatchs * nepochs\n",
        "\n",
        "    # Create the scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer = optimizer,\n",
        "        num_warmup_steps = 0, # Default value so direct training without warmup\n",
        "        num_training_steps = total_nb_steps)\n",
        "    \n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def evalute_Siamese(validation_loader, model, loss_fn, threshold, device):\n",
        "    \"\"\"Evaluate the performance of the SiameseBERTNet model on the validation dataset. \n",
        "    This function is specific to the SiameseBERTNet class.\n",
        "\n",
        "    Args:\n",
        "        validation_loader (torch.utils.data.dataloader.DataLoader): the validation dataloader\n",
        "        model (SiameseBERTNet): the SiameseBERT model to eval\n",
        "        loss_fn (ConstrastiveLoss): the initialized constrastive loss function\n",
        "        threshold (int): the threshold on the distance between 2 sentences. If D < th: duplicated (1), else non_duplicated (0)\n",
        "        device (torch.device): the device used by torch cpu or gpu.\n",
        "\n",
        "    Returns:\n",
        "        [float, float, float]: return the averaged loss, F1-score and accuracy on the validation dataset\n",
        "    \"\"\"\n",
        "    # Put model in test mode\n",
        "    model.eval()\n",
        "\n",
        "    epsilon = 1e-7\n",
        "\n",
        "    v_loss = []\n",
        "    v_f1 = []\n",
        "    v_accuracy = []\n",
        "\n",
        "    # Iterate over validation batches\n",
        "    for step, batch in enumerate(validation_loader):\n",
        "\n",
        "        # Get batch data\n",
        "        v_input_ids_Q1 = batch[0]['input_ids'].to(device)\n",
        "        v_attention_mask_Q1 = batch[0]['attention_mask'].to(device)\n",
        "        v_input_ids_Q2 = batch[1]['input_ids'].to(device)\n",
        "        v_attention_mask_Q2 = batch[1]['attention_mask'].to(device)\n",
        "        v_y = batch[2].to(device)\n",
        "\n",
        "        # Apply model\n",
        "        v_outQ1, v_outQ2 = model(v_input_ids_Q1, v_attention_mask_Q1, v_input_ids_Q2, v_attention_mask_Q2)\n",
        "\n",
        "        # Compute Loss\n",
        "        loss = loss_fn(v_outQ1, v_outQ2, v_y)\n",
        "        v_loss.append(loss.item())\n",
        "\n",
        "        # Compute prediction at m\n",
        "        pwdist = loss_fn.pdist(v_outQ1, v_outQ2)\n",
        "        pred = torch.where(pwdist < threshold, 1, 0)\n",
        "        \n",
        "        # Compute F1 score:\n",
        "        tp = (pred * v_y).cpu().numpy().sum()\n",
        "        precision = tp / (pred.cpu().numpy().sum() + epsilon)\n",
        "        recall = tp / (v_y.cpu().numpy().sum() + epsilon)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "        v_f1.append(f1)\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = (pred == v_y).cpu().numpy().mean() * 100\n",
        "        v_accuracy.append(accuracy)\n",
        "    \n",
        "    # Compute averaged loss\n",
        "    avg_loss = np.mean(v_loss)\n",
        "    avg_f1 = np.mean(v_f1)\n",
        "    avg_accuracy = np.mean(v_accuracy)\n",
        "\n",
        "    # return back the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    return avg_loss, avg_f1, avg_accuracy\n",
        "\n",
        "\n",
        "def train_loop_Siamese(model, dataloader, validation, optimizer, scheduler, loss_fn, eval_threshold, nepochs, device, out_dir):\n",
        "    \"\"\"The training loop for the SiameseBERTNet model. The function exports training logs to evaluate the model performances and overfitting during training.\n",
        "    The model is trained by minimizing the loss obtained with loss_fn.\n",
        "    If a validation dataloader is provided the model is evaluate on it using evalute_Siamese with 'loss_fn' and 'eval_threshold'. The evaluation is done every 'step' batch in each epoch and also at the end of each epoch. The results are saved in the returned training_logs dataframe. \n",
        "    The function also saves in the output directory the parameters of the best model obtained during training, based on the minimal evaluation loss that have been obtained (best-model.pt).\n",
        "    The best model can then be load using:\n",
        "    model = SiameseBERTNet(**params)\n",
        "    model.load_state_dict(torch.load(\"path/to/best-model.pt\"))\n",
        "    If no validation dataloader is provided (None) only the average training loss is reported and the parameters obtained at the end of training will be exported (not necessarily the best)\n",
        "    This model can be load using the same method as described above.\n",
        "\n",
        "    Args:\n",
        "        model (SiameseBERTNet): the SiameseBERTNet model to train\n",
        "        dataloader (torch.utils.data.dataloader.DataLoader): the training dataLoader\n",
        "        validation (torch.utils.data.dataloader.DataLoader): the validation dataloader\n",
        "        optimizer (transformers.AdamW): the parametrized optimizer from init_model\n",
        "        scheduler (torch.optim.lr_scheduler.LambdaLR): the parametrized scheduler from init_model\n",
        "        loss_fn (ConstrastiveLoss): the initialized constrastive loss function \n",
        "        eval_threshold (int): the threshold to use for evaluation\n",
        "        nepochs (int]): the number of training epochs (must be same as in the scheduler)\n",
        "        device (torch.device): the device used by torch cpu or gpu.\n",
        "        out_dir (str): the output directory\n",
        "\n",
        "    Returns:\n",
        "        [pd.DataFrame]: the training_logs dataframe. Reports the average training loss, evaluation loss, F1-score and accuracy computed on the state of the trained model every 'step' batchs in each epoch and also at the end of each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Init errors, F1-score and accuracy vector to store for all epochs\n",
        "    training_logs = pd.DataFrame()\n",
        "    _train_errors = []\n",
        "    _eval_errors = []\n",
        "    _eval_F1 = []\n",
        "    _eval_acc = []\n",
        "    _type = []\n",
        "\n",
        "    # Init evaluation measure for choosing the best model\n",
        "    best_model_eval_loss = np.inf\n",
        "\n",
        "    # Evaluation without training\n",
        "    if validation:\n",
        "        avg_validation_loss, avg_f1_validation, avg_acc_validation = evalute_Siamese(validation, model, loss_fn, eval_threshold, device)\n",
        "        _eval_errors.append(avg_validation_loss)\n",
        "        _eval_F1.append(avg_f1_validation)\n",
        "        _eval_acc.append(avg_acc_validation)\n",
        "        _train_errors.append(np.nan)\n",
        "        _type.append('init')\n",
        "        \n",
        "        # set the best model measure to avg_validation_loss\n",
        "        best_model_eval_loss = avg_validation_loss\n",
        "        print(\"Initial validation loss: \" + str(best_model_eval_loss))\n",
        "    \n",
        "    # Training on epochs\n",
        "    for i_epoch in range(nepochs):\n",
        "        print(\"-----------------------------------------------------------------------------------------------\")\n",
        "        # Init\n",
        "        total_step_in_dataloader = len(dataloader)\n",
        "        epoch_time, batch_time = time.time(), time.time()\n",
        "        total_loss, batch_loss, batch_count = 0, 0, 0\n",
        "\n",
        "        # Put model in train mode (important if a run on the validation in eval mode have been done previously)\n",
        "        model.train()\n",
        "\n",
        "        # iterate over batches:\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            \n",
        "            batch_count +=1\n",
        "\n",
        "            # Get batch data\n",
        "            input_ids_Q1 = batch[0]['input_ids'].to(device)\n",
        "            attention_mask_Q1 = batch[0]['attention_mask'].to(device)\n",
        "            input_ids_Q2 = batch[1]['input_ids'].to(device)\n",
        "            attention_mask_Q2 = batch[1]['attention_mask'].to(device)\n",
        "            y = batch[2].to(device)\n",
        "\n",
        "            # Reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Apply model\n",
        "            outQ1, outQ2 = model(input_ids_Q1, attention_mask_Q1, input_ids_Q2, attention_mask_Q2)\n",
        "\n",
        "            # Compute Constrastive loss\n",
        "            loss = loss_fn(outQ1, outQ2, y)\n",
        "\n",
        "            # Update batch loss and total loss\n",
        "            total_loss += loss.item()\n",
        "            batch_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Checking every 10 steps:\n",
        "            if ((((step + 1) % 25) == 0) and step != 0) or step == (total_step_in_dataloader - 1):\n",
        "                time_elapsed = str(timedelta(seconds = (time.time() - batch_time)))\n",
        "                batch_avg_loss = batch_loss/batch_count\n",
        "                _train_errors.append(batch_avg_loss)\n",
        "\n",
        "                if validation:\n",
        "                    avg_validation_loss, avg_f1_validation, avg_acc_validation = evalute_Siamese(validation, model, loss_fn, eval_threshold, device)\n",
        "                    _eval_errors.append(avg_validation_loss)\n",
        "                    _eval_F1.append(avg_f1_validation)\n",
        "                    _eval_acc.append(avg_acc_validation)\n",
        "                    \n",
        "                    # Print summary\n",
        "                    print(f\"batch {step+1:>6d} / {total_step_in_dataloader:>4d} | Elapsed {time_elapsed} | Average loss on the previous {batch_count:>4d} batchs : {batch_avg_loss:5.2f} | Average validation loss: {avg_validation_loss:6.2f} | Average F1-score: {avg_f1_validation:6.2f} | Average Accuracy: {avg_acc_validation:6.2f} %\")\n",
        "                else:\n",
        "                    print(f\"batch {step+1:>6d} / {total_step_in_dataloader:>4d} | Elapsed {time_elapsed} | Average loss on the previous {batch_count:>4d} batchs : {batch_avg_loss:5.2f} | \")\n",
        "                \n",
        "                _type.append(False)\n",
        "\n",
        "                # Reset batch_count, batch_loss and batch_time\n",
        "                batch_loss, batch_count = 0, 0\n",
        "                batch_time = time.time()\n",
        "        \n",
        "        avg_train_loss_epoch = total_loss/total_step_in_dataloader\n",
        "        _train_errors.append(avg_train_loss_epoch)\n",
        "\n",
        "        time_elapsed_epoch = str(timedelta(seconds = (time.time() - epoch_time)))\n",
        "        print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        # Test current model ( at epcoch i ) on validation\n",
        "        if validation:\n",
        "            avg_validation_loss, avg_f1_validation, avg_acc_validation = evalute_Siamese(validation, model, loss_fn, eval_threshold, device)\n",
        "            _eval_errors.append(avg_validation_loss)\n",
        "            _eval_F1.append(avg_f1_validation)\n",
        "            _eval_acc.append(avg_acc_validation)\n",
        "\n",
        "            # At the end of each epoch, check is the model has a better average evaluation loss than the previous best model:\n",
        "            if avg_validation_loss < best_model_eval_loss:\n",
        "                \n",
        "                print(\"\\n/!\\ save new best model /!\\ \\n\")\n",
        "                torch.save(model.state_dict(), os.path.join(out_dir, \"best-model.pt\"))\n",
        "\n",
        "                # Save best epoch\n",
        "                with open(os.path.join(out_dir, \"epoch.log\"), 'w') as f:\n",
        "                    f.write(\"best model at epoch: \" + str(i_epoch+1))\n",
        "                \n",
        "                best_model_eval_loss = avg_validation_loss\n",
        "        \n",
        "            print(f\"Epoch {i_epoch+1:>6d} / {nepochs:>4d} | Elapsed {time_elapsed_epoch} | Average loss on epoch: {avg_train_loss_epoch:18.2f} | Average validation loss: {avg_validation_loss:6.2f} | Average F1-score: {avg_f1_validation:6.2f} | Average Accuracy: {avg_acc_validation:6.2f} %\")\n",
        "        else:\n",
        "            print(f\"Epoch {i_epoch+1:>6d} / {nepochs:>4d} | Elapsed {time_elapsed_epoch} | Average loss on epoch: {avg_train_loss_epoch:18.2f} |\")\n",
        "        \n",
        "        _type.append(True)\n",
        "    \n",
        "    if validation:\n",
        "        training_logs = pd.DataFrame({\"Type\": _type, \"Training.loss\": _train_errors, \"Eval.loss\": _eval_errors, \"Eval.F1\": _eval_F1, \"Eval.Acc\": _eval_acc})\n",
        "    \n",
        "    else:\n",
        "        training_logs = pd.DataFrame({\"Type\": _type, \"Training.loss\": _train_errors})\n",
        "        \n",
        "        # if no validation was used, we just save the model at the end of the training\n",
        "        print(\"\\nsave last model\")\n",
        "        torch.save(model.state_dict(), os.path.join(out_dir, \"last-model.pt\"))\n",
        "\n",
        "    return training_logs\n",
        "\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "def cross_validation(model_params, dataset, k, loss_fn, eval_threshold, device, out_dir, batch_size = 8, nepochs = 4):\n",
        "    \"\"\"The function performs a cross-validation (CV) loop of the training set.\n",
        "    The training dataset is first split into k fold. For instance setting k to 5 means deviding the training into 5 distinct parts (index attributions is random) and then in each fold we use part as the validation set and the remaining as the training set.\n",
        "    In each fold we applied the train_loop_Siamese. and export the training_logs dataframe in the output directory\n",
        "\n",
        "    Args:\n",
        "        model_params (dict): a python dict containing the parameters to initialize a SiameseBERTNet model.\n",
        "        dataset (torch.utils.data.Dataset): the training dataset that will be used for CV\n",
        "        k (int): the number of fold\n",
        "        loss_fn (ConstrastiveLoss): the initialized constrastive loss function \n",
        "        eval_threshold (int): the threshold to use for evaluation\n",
        "        device (torch.device): the torch device cpu or gpu\n",
        "        out_dir (str): the output directory \n",
        "        batch_size (int, optional): the batch size\n",
        "        nepochs (int, optional): the number of training epochs (must be same as in the scheduler)\n",
        "    \"\"\"\n",
        "\n",
        "    # Init kfold: split the dataset into k folds. shuffle = True indicates that the individuals of the different folds are chosen randomly and are not necesseraly packs that follow each others\n",
        "    kfold = KFold(n_splits = k, shuffle = True)\n",
        "\n",
        "    # Loop over folds: At each steps, (k - 1) folds are chosen to be in the training set and the remaining kième fold is chose to be the validation set.\n",
        "    for fold, (train_ids, validation_ids) in enumerate(kfold.split(dataset)):\n",
        "        \n",
        "        # Check outdir\n",
        "        fold_out_dir = os.path.join(out_dir, \"f\" + str(fold + 1))\n",
        "        if not os.path.isdir(fold_out_dir):\n",
        "            os.makedirs(fold_out_dir)\n",
        "        \n",
        "        print(\" --- fold: \" + str(fold + 1) + \" --- \")\n",
        "        # To use the index of the individuals belonging to the (k - 1) training folds and the validation fold in the DataLoader, we create SubsetRandomSampler\n",
        "        # It creates a random sampler with the index in the (k - 1) training folds and the validation fold \n",
        "        train_subsampler = SubsetRandomSampler(train_ids)\n",
        "        validation_subsampler = SubsetRandomSampler(validation_ids)\n",
        "\n",
        "        # We then create 2 data loader: one for iterative with batches over the train_ids and the second for the validation_ids\n",
        "        # We don't need to use shuffle in the DataLoader as the selection of the ids is done with the provided samplers train_subsampler and validation_subsampler\n",
        "        train_loader = DataLoader(dataset, batch_size, sampler = train_subsampler)\n",
        "        validation_loader = DataLoader(dataset, batch_size, sampler = validation_subsampler)\n",
        "        \n",
        "        # Now, train the model with the train_loader and evaludate it on the validation_loader\n",
        "        model = SiameseBERTNet(**model_params)\n",
        "\n",
        "        # Check the parameters to be fine-tune\n",
        "        # check_params(model)\n",
        "\n",
        "        # Check for cuda:\n",
        "        model.to(device)\n",
        "        optimizer, scheduler = init_model(model, train_loader, nepochs)\n",
        "        k_training_logs = train_loop_Siamese(model, train_loader, validation_loader, optimizer, scheduler, loss_fn, eval_threshold, nepochs, device, fold_out_dir)\n",
        "        k_training_logs.to_csv(os.path.join(fold_out_dir, \"k_\" + str(fold + 1) + \"_training_logs.csv\"), index = True, header = True)\n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_Wji21dYOd5",
        "outputId": "8ea41dbe-36ee-4c58-a5c3-f3c23e806514"
      },
      "outputs": [],
      "source": [
        "_train = train.sample(frac = 1).reset_index(drop = True).head(n = 100)\n",
        "_train_dataset = SiameseNetWorkSentenceDataset(data = _train, tokenizer = tokenizer, max_length = 64)\n",
        "loss = ConstrastiveLoss(m = 10)\n",
        "eval_threshold = 5\n",
        "model_params = dict({'freeze_embedding': True, 'freeze_encoder_layer': 8, 'freeze_cls_pooler': True})\n",
        "out_dir = \"data/SiameseBERT/Trainingm10_th5_bs32\"\n",
        "cross_validation(model_params = model_params, dataset = _train_dataset, k = 5, loss_fn = loss, eval_threshold = eval_threshold, device = device, out_dir = out_dir, batch_size = 20, nepochs = 5)\n",
        "\n",
        "# TODO Faire des tests sur le thresold\n",
        "# TODO En fait pour tester le score au mieux, faudrait faire une AUC !\n",
        "# TODO: il faudrait shuffle le dataset train AVANT de prendre les 10,000 premières lignes ! . On fiat nos test CV sur juste 10k, on détermine le meilleur (car c'est déjà 3h de run ...) et ensuite on fait tourner sur full !\n",
        "# TODO tester batch: 64 & freeze 10 layers ?\n",
        "\n",
        "# For tests\n",
        "# model = SiameseBERTNet()\n",
        "\n",
        "# test_train_dataloader = DataLoader(test_train_dataset, batch_size = 8, shuffle = True, num_workers = 0)\n",
        "\n",
        "# nepochs = 4\n",
        "# \n",
        "# optimizer, scheduler = init_model(model, test_train_dataloader, nepochs)\n",
        "# train_loop_Siamese(model, test_train_dataloader, optimizer, scheduler, loss, nepochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour le modèle final, ce qui semble le mieux c'est de le train également en CV en utilisant toutes les donnée cette fois-ci afin de pouvoir capter a poseriori le moment où celui-ci commence à overfitter. Néanmoins, afin de conserver une convertion du modèle avant l'overfit on propose de faire une sauvegarde des paramètres du modèle après chaque epoch de tel sorte qu'après le training et analyse de la loss Train/Eval on pourra revenir à une version du modèle où il n'avait pas encore over-fit. Il faut suivre la démarche suivante dans :\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "\"If you only plan to keep the best performing model (according to the acquired validation loss), don’t forget that best_model_state = model.state_dict() returns a reference to the state and not its copy! You must serialize best_model_state or use best_model_state = deepcopy(model.state_dict()) otherwise your best best_model_state will keep getting updated by the subsequent training iterations. As a result, the final model state will be the state of the overfitted model.\"\n",
        "\n",
        "Après ATTENTION lorsque l'on va train le modèle final je pense que l'on va faire que 1 fold: séparer notre training en mode 90% train, 10 % validation et faire la CV la dessus mais pas faire plusieurs fold ! Ça on le fait juste pour l'optimisation des paramètres ! "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook_qqp.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "19c1444b3d5567e4cbdf4788d938f6edea2231b6a36cad91fb20378c11783a94"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('QQP': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0adb275221cc494fa02cae32615faa06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a237809fedf44dd4b757702b0440dc86",
            "placeholder": "​",
            "style": "IPY_MODEL_5e9b53af0900470f8750a714c7a54792",
            "value": "Downloading: 100%"
          }
        },
        "0b264b8009fb4e6396b7a288efd09b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d72911f868f43029c37562e1dc80184": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_878bf3134b5e438fa67aea29229295ee",
              "IPY_MODEL_37aadca028d2455aac1d16a3359d80ed",
              "IPY_MODEL_5b7bdd842e534a32b119d820f80ba5d2"
            ],
            "layout": "IPY_MODEL_e8c70c2d3e4a4427887d3bbae07b4d57"
          }
        },
        "15cd267d46d54cbfa9efb59495cd5166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a81bb574a2f4c58bb461f2ca1a37512": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7596d4503284ef5940c9aedf6e9f87e",
            "placeholder": "​",
            "style": "IPY_MODEL_0b264b8009fb4e6396b7a288efd09b6d",
            "value": " 226k/226k [00:00&lt;00:00, 851kB/s]"
          }
        },
        "1c7566e70e944859b80bb72d4ac059fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ebb13485eec4e7290be4c6d8d9b4e76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3453ebcf84834764a9c4c49a1afae2db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3662c943e30c465fb0a2458926abfd18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37aadca028d2455aac1d16a3359d80ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfc8a047c03c428f97ac1f3dc6926b07",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3662c943e30c465fb0a2458926abfd18",
            "value": 466062
          }
        },
        "45f2e04395194ce49d174551c29a7f34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4bb7333b104e0d85193d9d298d76d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f300a0dbc0a492f83fd013996992169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0adb275221cc494fa02cae32615faa06",
              "IPY_MODEL_9be4143ba87d4d82b8a132cb02f3b8cd",
              "IPY_MODEL_67b951a6eefb4c029272cd4edd6c9273"
            ],
            "layout": "IPY_MODEL_92cabeaf24ff480a9fab4764656fd3ac"
          }
        },
        "5b7bdd842e534a32b119d820f80ba5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ebb13485eec4e7290be4c6d8d9b4e76",
            "placeholder": "​",
            "style": "IPY_MODEL_672b58f79dee4932acd3315486a12ffa",
            "value": " 455k/455k [00:00&lt;00:00, 920kB/s]"
          }
        },
        "5e9b53af0900470f8750a714c7a54792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "672b58f79dee4932acd3315486a12ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67b951a6eefb4c029272cd4edd6c9273": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bc4ba86839f4d85be9331be2e931c55",
            "placeholder": "​",
            "style": "IPY_MODEL_939c7be78816400794f6731b6a53c770",
            "value": " 570/570 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "74aef20f9dc44a12a70bb90f209c90a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe3222259799457498476f71156a6af0",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15cd267d46d54cbfa9efb59495cd5166",
            "value": 231508
          }
        },
        "7fb441505c694bfc9e33d1d0110e15e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "820067dd1926477d82b351a17e57031d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85ce88f2fcf74953886d073b936cd64e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878bf3134b5e438fa67aea29229295ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ce88f2fcf74953886d073b936cd64e",
            "placeholder": "​",
            "style": "IPY_MODEL_a146b69f1962430183d9915dce65f0a9",
            "value": "Downloading: 100%"
          }
        },
        "8a700435f348487f8fe6a084eba64b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bc4ba86839f4d85be9331be2e931c55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c0623aacf9c4bb0a999a4909e576781": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3453ebcf84834764a9c4c49a1afae2db",
            "placeholder": "​",
            "style": "IPY_MODEL_c01edc2c53cd4be1b8157bb165b7a8d9",
            "value": " 28.0/28.0 [00:00&lt;00:00, 720B/s]"
          }
        },
        "92cabeaf24ff480a9fab4764656fd3ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "939c7be78816400794f6731b6a53c770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9be4143ba87d4d82b8a132cb02f3b8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f90bd973cb94de881c245669e9f5aa4",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b28833bc6b8547a78ed2f2fd548280fd",
            "value": 570
          }
        },
        "9de5e2fc54644c8faf82c75b4e68fab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab9a3ca098c04d48976158db70b4ad85",
              "IPY_MODEL_be37868d6c1c4ee1a0886acfa1215f1e",
              "IPY_MODEL_8c0623aacf9c4bb0a999a4909e576781"
            ],
            "layout": "IPY_MODEL_1c7566e70e944859b80bb72d4ac059fe"
          }
        },
        "9f90bd973cb94de881c245669e9f5aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a146b69f1962430183d9915dce65f0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a237809fedf44dd4b757702b0440dc86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9a3ca098c04d48976158db70b4ad85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45f2e04395194ce49d174551c29a7f34",
            "placeholder": "​",
            "style": "IPY_MODEL_820067dd1926477d82b351a17e57031d",
            "value": "Downloading: 100%"
          }
        },
        "b28833bc6b8547a78ed2f2fd548280fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7596d4503284ef5940c9aedf6e9f87e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be37868d6c1c4ee1a0886acfa1215f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fb441505c694bfc9e33d1d0110e15e5",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebc0d675b11b40a98da35b0b77ca2a18",
            "value": 28
          }
        },
        "c01edc2c53cd4be1b8157bb165b7a8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0f103dcb8f643c8af52411dfc7dbb51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b4bb7333b104e0d85193d9d298d76d3",
            "placeholder": "​",
            "style": "IPY_MODEL_8a700435f348487f8fe6a084eba64b4b",
            "value": "Downloading: 100%"
          }
        },
        "cfc8a047c03c428f97ac1f3dc6926b07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dacf674654154cdfb8162435a97d941f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db076208f0464ebea6be7b842fba0dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0f103dcb8f643c8af52411dfc7dbb51",
              "IPY_MODEL_74aef20f9dc44a12a70bb90f209c90a2",
              "IPY_MODEL_1a81bb574a2f4c58bb461f2ca1a37512"
            ],
            "layout": "IPY_MODEL_dacf674654154cdfb8162435a97d941f"
          }
        },
        "e8c70c2d3e4a4427887d3bbae07b4d57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc0d675b11b40a98da35b0b77ca2a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe3222259799457498476f71156a6af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
