{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import statistics\n",
    "from transformers import BertTokenizer, BertForPreTraining, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilitary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataset(dataset):\n",
    "    \n",
    "    # Check is all questions in 'question1' and 'question2' are str\n",
    "    filter = np.array([not isinstance(s1, str) for s1 in dataset['question1'].tolist()]) | np.array([not isinstance(s2, str) for s2 in dataset['question2'].tolist()])\n",
    "    indexes_to_drop = dataset[filter].index\n",
    "    \n",
    "    # drop lines that are not\n",
    "    if not len(indexes_to_drop):\n",
    "        print(\"All rows are corrects\")\n",
    "    else:\n",
    "        print(\"Removing the following lines: \")\n",
    "        print(dataset.loc[indexes_to_drop])\n",
    "        dataset = dataset.drop(indexes_to_drop)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the following lines: \n",
      "          qid1    qid2                         question1  \\\n",
      "id                                                         \n",
      "105780  174363  174364    How can I develop android app?   \n",
      "201841  303951  174364  How can I create an Android app?   \n",
      "363362  493340  493341                               NaN   \n",
      "\n",
      "                                                question2  is_duplicate  \n",
      "id                                                                       \n",
      "105780                                                NaN             0  \n",
      "201841                                                NaN             0  \n",
      "363362  My Chinese name is Haichao Yu. What English na...             0  \n",
      "Removing the following lines: \n",
      "                                           question1  \\\n",
      "test_id                                                \n",
      "379205      How I can learn android app development?   \n",
      "817520   How real can learn android app development?   \n",
      "943911                          How app development?   \n",
      "1046690                                          NaN   \n",
      "1270024             How I can learn app development?   \n",
      "1461432                                          NaN   \n",
      "\n",
      "                                               question2  \n",
      "test_id                                                   \n",
      "379205                                               NaN  \n",
      "817520                                               NaN  \n",
      "943911                                               NaN  \n",
      "1046690    How I what can learn android app development?  \n",
      "1270024                                              NaN  \n",
      "1461432  How distinct can learn android app development?  \n"
     ]
    }
   ],
   "source": [
    "# Paths & Variables\n",
    "\n",
    "data_path = \"data/quora-question-pairs\"\n",
    "train_file = \"train.csv\"\n",
    "test_pos_file = \"test.csv\"\n",
    "label_file = \"sample_submission.csv\"\n",
    "\n",
    "# Reading\n",
    "train = pd.read_csv(os.path.join(data_path, train_file), index_col = 0)\n",
    "test_pos = pd.read_csv(os.path.join(data_path, test_pos_file), index_col = 0)\n",
    "y_label = pd.read_csv(os.path.join(data_path, label_file), index_col = 0)\n",
    "\n",
    "# Fix datasets for NaN values in question1 or question2\n",
    "train = fix_dataset(train)\n",
    "test_pos = fix_dataset(test_pos)\n",
    "\n",
    "# join test and y_label\n",
    "test_pos = test_pos.join(y_label, on = 'test_id', how = 'left')\n",
    "\n",
    "# test set contains only positive, labels; suffle to create negative examples\n",
    "test_neg = test_pos.copy()\n",
    "test_neg['question1'] = np.random.permutation(test_neg['question1'])\n",
    "test_neg['is_duplicate'] = 0\n",
    "\n",
    "# Create final test set\n",
    "test = pd.concat([test_pos, test_neg], ignore_index = True)\n",
    "\n",
    "# Reset indexes\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "test.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "\n",
    "See (https://paperswithcode.com/method/wordpiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id for [CLS]: 101\n",
      "Token id for [SEP]: 102\n",
      "Token id for [PAD]: 0\n",
      "Token id for [UNK]: 100\n",
      "Token id for [MASK]: 103\n",
      "Original sentense: What is the step by step guide to invest in share market in india?\n",
      "Encoded sentense: \n",
      "[101, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1999, 2634, 1029, 102]\n",
      "Decoded sentense: \n",
      "[CLS] what is the step by step guide to invest in share market in india? [SEP]\n",
      "La taille maximale de tokens est 286 (avec les [CLS] et [SEP])\n",
      "Il y a 99.88% des phrases tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\n",
      "La taille maximale de tokens avec paires mergée est 330 (avec les [CLS] et [SEP])\n",
      "Il y a 97.91% des paires de phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\n",
      "Il y a 99.97% des paires de  phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Token id for [CLS]: \" + str(tokenizer.cls_token_id))\n",
    "print(\"Token id for [SEP]: \" + str(tokenizer.sep_token_id))\n",
    "print(\"Token id for [PAD]: \" + str(tokenizer.pad_token_id))\n",
    "print(\"Token id for [UNK]: \" + str(tokenizer.unk_token_id))\n",
    "print(\"Token id for [MASK]: \" + str(tokenizer.mask_token_id))\n",
    "\n",
    "print(\"Original sentense: \" + train.loc[0, 'question1'])\n",
    "print(\"Encoded sentense: \")\n",
    "enc = tokenizer.encode(train.loc[0, 'question1'])\n",
    "print(enc)\n",
    "print(\"Decoded sentense: \")\n",
    "dec = tokenizer.decode(enc)\n",
    "print(dec)\n",
    "\n",
    "# Check len of tokenized training sentences:\n",
    "list_len = []\n",
    "all_s = train['question1'].tolist() + train['question2'].tolist()\n",
    "for s in all_s:\n",
    "    tks = tokenizer.encode(s)\n",
    "    list_len.append(len(tks))\n",
    "\n",
    "max_len = max(list_len)\n",
    "\n",
    "print(f\"La taille maximale de tokens est {max_len} (avec les [CLS] et [SEP])\")\n",
    "lw_64 = round((sum([l <= 64 for l in list_len])/len(list_len)) * 100, 2) \n",
    "print(f\"Il y a {lw_64}% des phrases tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\")\n",
    "\n",
    "# Check for the sentences pairs\n",
    "list_len2 = []\n",
    "for i in range(train.shape[0]):\n",
    "    tks = tokenizer.encode(train['question1'][i], train['question2'][i])\n",
    "    list_len2.append(len(tks))\n",
    "\n",
    "max_len2 = max(list_len2)\n",
    "print(f\"La taille maximale de tokens avec paires mergée est {max_len2} (avec les [CLS] et [SEP])\")\n",
    "lw_64 = round((sum([l <= 64 for l in list_len2])/len(list_len2)) * 100, 2)\n",
    "lw_128 = round((sum([l <= 128 for l in list_len2])/len(list_len2)) * 100, 2)\n",
    "print(f\"Il y a {lw_64}% des paires de phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\")\n",
    "print(f\"Il y a {lw_128}% des paires de  phrases mergées tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "See https://pytorch.org/tutorials/beginner/basics/data_tutorial.html for documentation about the Dataset and Dataloader creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetWorkSentenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SiameseNetWorkSentenceDataset create a Dataset\n",
    "    - data (pd.DataFrame): the data dataframe with column 'question1' and 'question2' along with the label 'is_duplicate'\n",
    "    - tokenizer: the BERT tokenizer, such as: BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    - max_length: the maximal length of tokens input vector (default 64) Shorter vector arre padded to max_length with [PAD token] (id: 0) and longer are truncated. \n",
    "    The size includes the start [CLS] and end [SEP] tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        def squeeze_tensors(tks):\n",
    "            \"\"\"Take a tensor and remove unnecessary dimension. When using tokenizer with return_tensors = 'pt', the returned tensor is by default 2 dimensions, has it could handle a list of sentence as inputs.\n",
    "            However, as we only sent one sentence at a time to the tokenizer to create the Dataset, it result in an additional dimension that will be useless after pooling results by batches in the DataLoader\n",
    "\n",
    "            Args:\n",
    "                tks ([type]): [description]\n",
    "            \"\"\"\n",
    "            tks.data[\"input_ids\"] = torch.squeeze(tks.data[\"input_ids\"])\n",
    "            tks.data[\"token_type_ids\"] = torch.squeeze(tks.data[\"token_type_ids\"])\n",
    "            tks.data[\"attention_mask\"] = torch.squeeze(tks.data[\"attention_mask\"])\n",
    "\n",
    "        s1 = self.data.loc[index, 'question1']\n",
    "        s2 = self.data.loc[index, 'question2']\n",
    "        label = torch.tensor(self.data.loc[index, 'is_duplicate'])\n",
    "\n",
    "        tokens1 = self.tokenizer(text = s1, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "        squeeze_tensors(tokens1)\n",
    "        tokens2 = self.tokenizer(text = s2, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "        squeeze_tensors(tokens2)\n",
    "\n",
    "        return tokens1, tokens2, label\n",
    "\n",
    "\n",
    "class BERTSentencesClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        def squeeze_tensors(tks):\n",
    "            \"\"\"Take a tensor and remove unnecessary dimension. When using tokenizer with return_tensors = 'pt', the returned tensor is by default 2 dimensions, has it could handle a list of sentence as inputs.\n",
    "            However, as we only sent one sentence at a time to the tokenizer to create the Dataset, it result in an additional dimension that will be useless after pooling results by batches in the DataLoader\n",
    "\n",
    "            Args:\n",
    "                tks ([type]): [description]\n",
    "            \"\"\"\n",
    "            tks.data[\"input_ids\"] = torch.squeeze(tks.data[\"input_ids\"])\n",
    "            tks.data[\"token_type_ids\"] = torch.squeeze(tks.data[\"token_type_ids\"])\n",
    "            tks.data[\"attention_mask\"] = torch.squeeze(tks.data[\"attention_mask\"])\n",
    "        \n",
    "        s1 = self.data.loc[index, 'question1']\n",
    "        s2 = self.data.loc[index, 'question2']\n",
    "\n",
    "        t = self.tokenizer(s1, s2, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "        squeeze_tensors(t)\n",
    "\n",
    "        label = torch.tensor(self.data.loc[index, 'is_duplicate'])\n",
    "\n",
    "        return t, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([  101,  4118,  2000,  2424,  8745,  1997, 29199,  2478, 10424,  2229,\n",
      "        11877, 12170, 18098,  2964,  1029,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, {'input_ids': tensor([  101,  2054,  2024,  2070,  1997,  1996,  2477, 20202,  2064,  2425,\n",
      "         2055,  1996,  4241,  2527,  8553,  1998, 15258,  1997, 12191,  2015,\n",
      "         1998,  2049,  6177,  1029,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, tensor(0))\n",
      "404287\n",
      "({'input_ids': tensor([  101,  4118,  2000,  2424,  8745,  1997, 29199,  2478, 10424,  2229,\n",
      "        11877, 12170, 18098,  2964,  1029,   102,  2054,  2024,  2070,  1997,\n",
      "         1996,  2477, 20202,  2064,  2425,  2055,  1996,  4241,  2527,  8553,\n",
      "         1998, 15258,  1997, 12191,  2015,  1998,  2049,  6177,  1029,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, tensor(0))\n",
      "404287\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = SiameseNetWorkSentenceDataset(data = train, tokenizer = tokenizer, max_length = 64)\n",
    "print(train_dataset[10])\n",
    "print(len(train_dataset))\n",
    "\n",
    "train_dataset_2 = BERTSentencesClassificationDataset(data = train, tokenizer = tokenizer, max_length = 64)\n",
    "print(train_dataset_2[10])\n",
    "print(len(train_dataset_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size = 8, shuffle = True, num_workers = 0)\n",
    "dataloader_2 = DataLoader(train_dataset_2, batch_size = 8, shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset details\n",
    "\n",
    "Each input in the training/test dataset is composed of question1, question2, and label\n",
    "The Dataset in shuffled / divided in batchs of size *batch_size* in the DataLoader.\n",
    "\n",
    "## SiameseNetWorkSentenceDataset\n",
    "\n",
    "For the siamese network BERT model, a batch is a tuple of 3 elements: (*batch_question_1*, *batch_question_2*, *label*).\n",
    "\n",
    "For the *batch_size* pair of questions in the created batch: \n",
    "- *batch_question_1*: contains the BERT-tokenizer's output of each question1\n",
    "- *batch_question_2*: contains the BERT-tokenizer's output of each question2\n",
    "- *label* indicates if the two questions are duplicated \n",
    "\n",
    "*batch_question_1* and *batch_question_2* are similar and contains a dictionary with 3 entries:\n",
    "\n",
    "- \"input_ids\": torch.FloatTensor of shape (*batch_size*, *sequence_length*) which contains the indices of the question tokens in the vocabulary, with [CLS] and [SEP] at the beginning and end of the sentence and the [PAD] token for the remaining padding (up to *max_length*).\n",
    "\n",
    "- \"token_type_ids\": torch.FloatTensor of shape (*batch_size*, *sequence_length*) which are the segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]. Always 0 here as the two questions are treated separately in the siamese network.\n",
    "\n",
    "- attention_mask torch.FloatTensor of shape (*batch_size*, *sequence_length* which contains the mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "\n",
    "When calling the model SiameseBERTNet, *batch_question_1* is fed in the 'left' part of the siamese and *batch_question_2* is fed in the right part of the siamese. In each siamese, we compute for each question the average of the last hidden layer along each 'real' token of the question, so that we don't average with [PAD], [CLS] and [SEP] tokens.\n",
    "At the end, for each *batch_size* pair of questions sent to the siamese Network, it returns a tuple with the two set of averaged vectors for question1 and question2, noted (OUT1, OUT2), each of shape (*batch_size*, dmodel). dmodel of BERT is 768. The ième line in OUT1 correspond to the averaged output hidden layer for the ième question1 in *batch_question_1*.\n",
    "\n",
    "In the loss function, we want to minimze the distance between 2 averaged vector OUT1[i, ] and OUT2[i, ] if they are duplicated and maximize the distance if they are not duplicated\n",
    "\n",
    "## BERTSentencesClassification\n",
    "\n",
    "For BERTSentence classification we choose to directly use the [CLS] token as a predictor like it is done for the NextSentencePrediction task.\n",
    "Each batch is a tuple of 2 elements (*batch_pair_of_questions*, *label*).\n",
    "\n",
    "*batch_pair_of_question* is a dictionary like *batch_question_1* or *batch_question_1* with each elements (input_ids, token_type_ids, attention_mask) of shape (*batch_size*, *sequence_length*), except that here the two question have been concatened into one sequence, such as : [CLS] [... TOKEN Q1 ...] [SEP] [... TOKEN Q2 ...] [SEP]. In this case *token_type_ids* matrix is important as we have to distinguish the both sentences in the embedding. The advantage of this approach is that the attention of each tokens is computed over all the tokens of the sequence, included those of the other question, while the siamese network compute the averaged output token independenty for each question.\n",
    "\n",
    "After being fed into the BERT model, the outputed vector corresponding to the [CLS] token, namely pooler_output, in sent into a dropout, linear layer with two outputed dimension and finally a softmax. The goal is to predict is the two concatenated questions are duplicated or not. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBERTNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseBERTNet, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.HS = self.bert.config.hidden_size\n",
    "\n",
    "    def forward_siamese(self, input, noCLSpooling = True, noSEPpooling = True):\n",
    "        \"\"\"From tokenised input sentence, compute BERT \n",
    "\n",
    "        Args:\n",
    "            input (dict): output dict from the tokenizer with input_ids, token_type_ids and attention_mask\n",
    "\n",
    "        Returns:\n",
    "            avg (tensor): Mean of the last hidden layer vectors for real tokens (attention_mask: 1) in the input.\n",
    "        \"\"\"\n",
    "        # Get input_ids and attention mask\n",
    "        input_ids, token_type_ids, attention_mask = input.values()\n",
    "\n",
    "        # Apply BERT and extract last_hidden_state\n",
    "        out = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        last_hidden_state = out.last_hidden_state\n",
    "\n",
    "        # Apply mean pooling on real tokens\n",
    "        # Make a copy is some changes (CLS or SEP) need to be applied\n",
    "        pooling_mask = attention_mask.detach().clone()\n",
    "\n",
    "        # If the CLS output vector should not participate in average pooling\n",
    "        if noCLSpooling:\n",
    "            pooling_mask[:, 0] = 0\n",
    "        \n",
    "        # If the SEP output vector should not participate in average pooling\n",
    "        if noSEPpooling:\n",
    "            pooling_mask = torch.where(input_ids == 102, 0, pooling_mask)\n",
    "\n",
    "        # Get mask at the same dimension as last_hidden_state\n",
    "        expanded_pooling_mask = pooling_mask.unsqueeze(-1)\n",
    "        expanded_pooling_mask = expanded_pooling_mask.expand(-1, -1, self.HS)\n",
    "\n",
    "        # Element wise mul between last_hidden_state and mask to then only consider real tokens in the sum\n",
    "        prod = torch.mul(last_hidden_state, expanded_pooling_mask)\n",
    "\n",
    "        # Sum all token vectors\n",
    "        sum_by_tks = torch.sum(prod, dim = 1)\n",
    "\n",
    "        # Get normalisation factor to compute mean\n",
    "        norm = torch.sum(pooling_mask, dim = -1).unsqueeze(-1)\n",
    "\n",
    "        # Comptue average\n",
    "        avg = torch.div(sum_by_tks, norm)\n",
    "\n",
    "        return avg\n",
    "\n",
    "\n",
    "    def forward(self, question1, question2):\n",
    "        out1 = self.forward_siamese(question1)\n",
    "        out2 = self.forward_siamese(question2)\n",
    "\n",
    "        return out1, out2\n",
    "\n",
    "class BERTSentencesClassification(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERTSentencesClassification, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.HS = self.bert.config.hidden_size\n",
    "        self.out = 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.2),\n",
    "            nn.Linear(in_features = self.HS, out_features = self.out, bias = True),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, sentensepair):\n",
    "\n",
    "        # Get input_ids, token_type_ids (as we have sentense pairs) and attention mask\n",
    "        out = self.bert(**sentensepair)\n",
    "        cls_token = out.pooler_output\n",
    "        classification = self.classifier(cls_token)\n",
    "\n",
    "        return classification, cls_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also see https://skimai.com/fine-tuning-bert-for-sentiment-analysis/ for tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SiameseBERTNet()\n",
    "model2 = BERTSentencesClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details about the last 'real' layer BertPooler see:  https://github.com/huggingface/transformers/issues/782 and https://github.com/google-research/bert/issues/43. Il s'agit d'une tranformation appliquée uniquement au token [CLS]. L'article ne détaille pas tout mais en réalité il y a donc un layer linéaire de transformation appliquée sur le token CLS avant de l'envoyé dans un linear layer de classification/softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = next(iter(dataloader))\n",
    "r = model(s1[0], s1[1])\n",
    "s2 = next(iter(dataloader_2))\n",
    "r_2 = model2(s2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BERT embedding without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "How does the Surface Pro himself 4 compare with iPad Pro?"
          ],
          [
           "Should I have a hair transplant at age 24? How much would it cost?"
          ],
          [
           "What but is the best way to send money from China to the US?"
          ],
          [
           "Which food not emulsifiers?"
          ],
          [
           "How \"aberystwyth\" start reading?"
          ],
          [
           "How are the two wheeler insurance from Bharti Axa insurance?"
          ],
          [
           "How can I reduce my belly fat through a diet?"
          ],
          [
           "By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?"
          ],
          [
           "What are the how best books of all time?"
          ],
          [
           "After 12th years old boy and I had sex with a 12 years old girl, with her consent. Is there anything wrong?"
          ],
          [
           "When are some tips to get rid of lizards?"
          ],
          [
           "Which college is better eat CSE in chennai?"
          ],
          [
           "How is the word 4 used in a sentence?"
          ],
          [
           "Where is fingerprint involve of LEO Privacy Guard? How does it work?"
          ],
          [
           "What were the major doing contributions of the political leaders during WW1, and how are the compared to the ones during the Austro-Prussian War?"
          ],
          [
           "I'm having a severe acne problem. psychopaths there any way to cure it?"
          ],
          [
           "How is was brass welded onto steel?"
          ],
          [
           "Who are some Brazilians dislike their Carnivals?"
          ],
          [
           "What are the new strategies to promote longer app?"
          ],
          [
           "What is the best way to learn new support to add to your vocabulary?"
          ],
          [
           "Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?"
          ],
          [
           "How much cost does hair transplant require?"
          ],
          [
           "What you send money to China?"
          ],
          [
           "What foods fibre?"
          ],
          [
           "How their can I start reading?"
          ],
          [
           "I admire I am considering of buying insurance from them"
          ],
          [
           "How can I reduce my lower belly fat in one month?"
          ],
          [
           "How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?"
          ],
          [
           "What are some of the military history books of all time?"
          ],
          [
           "Can a 14 old guy date a 12 year old girl?"
          ],
          [
           "What are the hair for my birthday?"
          ],
          [
           "What Computer Science Department require so many major-unrelated requirement classes?"
          ],
          [
           "Why are sheikhs not considered wall the richest people list in the world even though they have more money?"
          ],
          [
           "I am working in an IT company with 9 hours side work. Is it possible for me to crack the GATE in electrical engineering?"
          ],
          [
           "What are the types of models used in Cost Centers?"
          ],
          [
           "What is the most study scene in twin peaks?"
          ],
          [
           "How question FedEx packages delivered?"
          ],
          [
           "Can a non-alcoholic restaurant be a huge success?"
          ],
          [
           "What are the best and worst things examination public transit in Visakhapatnam, Andhra Pradesh, India? How could it be improved?"
          ],
          [
           "How do I out get rid of Erectile Dysfunction?"
          ]
         ],
         "hovertemplate": "X=%{x}<br>Y=%{y}<br>question=%{customdata[0]}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.6007058620452881,
          2.669735908508301,
          -1.575535535812378,
          0.17832596600055695,
          0.2076728641986847,
          -0.47587212920188904,
          3.2299437522888184,
          -0.4070180058479309,
          -2.8828749656677246,
          2.4616456031799316,
          0.18136298656463623,
          -0.06446115672588348,
          -0.5135779976844788,
          -0.17315702140331268,
          -1.8060193061828613,
          2.1930840015411377,
          -1.2020683288574219,
          -0.6603677272796631,
          -1.5122668743133545,
          -0.40581995248794556,
          -0.6727268099784851,
          1.0427519083023071,
          -2.683685779571533,
          -1.4521057605743408,
          1.123497724533081,
          0.11723637580871582,
          3.201434373855591,
          -0.6465027928352356,
          -2.706296443939209,
          2.9638137817382812,
          0.31562167406082153,
          -0.09624749422073364,
          -0.8048787117004395,
          1.5664329528808594,
          -2.1226844787597656,
          -0.38188737630844116,
          -1.918406367301941,
          2.4808130264282227,
          0.1959095150232315,
          3.041076183319092
         ],
         "xaxis": "x",
         "y": [
          -0.3378978371620178,
          2.0336897373199463,
          -0.2039158046245575,
          1.5812923908233643,
          0.03245849907398224,
          -1.458979845046997,
          2.9927992820739746,
          -1.5867841243743896,
          3.3123204708099365,
          -2.1333510875701904,
          1.9135512113571167,
          0.02415364980697632,
          0.45804429054260254,
          0.05541199445724487,
          -0.043898239731788635,
          -0.005716055631637573,
          1.2400267124176025,
          0.95055091381073,
          1.0033442974090576,
          1.4686354398727417,
          -2.09505558013916,
          2.4314634799957275,
          1.1718608140945435,
          3.4295754432678223,
          4.066810607910156,
          -0.8794410228729248,
          2.668271780014038,
          -0.6697096228599548,
          2.3664796352386475,
          -1.1563372611999512,
          4.048718452453613,
          0.10105928778648376,
          -0.19715449213981628,
          -0.9111619591712952,
          0.9848931431770325,
          1.135283350944519,
          0.9475057721138,
          0.39566153287887573,
          -0.6638723015785217,
          2.2213375568389893
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "X"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the Surface Pro himself 4 compare with iPad Pro? -> closest: Where is fingerprint involve of LEO Privacy Guard? How does it work?. Real label is: Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\n",
      "Should I have a hair transplant at age 24? How much would it cost? -> closest: How much cost does hair transplant require?. Real label is: How much cost does hair transplant require?\n",
      "What but is the best way to send money from China to the US? -> closest: What you send money to China?. Real label is: What you send money to China?\n",
      "Which food not emulsifiers? -> closest: What foods fibre?. Real label is: What foods fibre?\n",
      "How \"aberystwyth\" start reading? -> closest: What are the best and worst things examination public transit in Visakhapatnam, Andhra Pradesh, India? How could it be improved?. Real label is: How their can I start reading?\n",
      "How are the two wheeler insurance from Bharti Axa insurance? -> closest: By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?. Real label is: I admire I am considering of buying insurance from them\n",
      "How can I reduce my belly fat through a diet? -> closest: How can I reduce my lower belly fat in one month?. Real label is: How can I reduce my lower belly fat in one month?\n",
      "By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money? -> closest: How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?. Real label is: How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?\n",
      "What are the how best books of all time? -> closest: What are some of the military history books of all time?. Real label is: What are some of the military history books of all time?\n",
      "After 12th years old boy and I had sex with a 12 years old girl, with her consent. Is there anything wrong? -> closest: I am working in an IT company with 9 hours side work. Is it possible for me to crack the GATE in electrical engineering?. Real label is: Can a 14 old guy date a 12 year old girl?\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "Q1: How does the Surface Pro himself 4 compare with iPad Pro?\nQ2: Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?"
          ],
          [
           "Q1: Should I have a hair transplant at age 24? How much would it cost?\nQ2: How much cost does hair transplant require?"
          ],
          [
           "Q1: What but is the best way to send money from China to the US?\nQ2: What you send money to China?"
          ],
          [
           "Q1: Which food not emulsifiers?\nQ2: What foods fibre?"
          ],
          [
           "Q1: How \"aberystwyth\" start reading?\nQ2: How their can I start reading?"
          ],
          [
           "Q1: How are the two wheeler insurance from Bharti Axa insurance?\nQ2: I admire I am considering of buying insurance from them"
          ],
          [
           "Q1: How can I reduce my belly fat through a diet?\nQ2: How can I reduce my lower belly fat in one month?"
          ],
          [
           "Q1: By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?\nQ2: How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?"
          ],
          [
           "Q1: What are the how best books of all time?\nQ2: What are some of the military history books of all time?"
          ],
          [
           "Q1: After 12th years old boy and I had sex with a 12 years old girl, with her consent. Is there anything wrong?\nQ2: Can a 14 old guy date a 12 year old girl?"
          ]
         ],
         "hovertemplate": "label=1<br>X=%{x}<br>Y=%{y}<br>question=%{customdata[0]}<extra></extra>",
         "legendgroup": "1",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "1",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -19.838850021362305,
          -19.833271026611328,
          -19.32789421081543,
          -18.42644500732422,
          -18.207679748535156,
          -19.029354095458984,
          -20.241191864013672,
          -20.279741287231445,
          -19.311294555664062,
          -20.09279441833496
         ],
         "xaxis": "x",
         "y": [
          -5.0453691482543945,
          -5.092106819152832,
          -5.139787197113037,
          -5.5669450759887695,
          -5.340473651885986,
          -5.2486467361450195,
          -4.941092491149902,
          -4.8760552406311035,
          -5.181333541870117,
          -5.015231609344482
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Q1: When are some tips to get rid of lizards?\nQ2: What are the hair for my birthday?"
          ],
          [
           "Q1: Which college is better eat CSE in chennai?\nQ2: What Computer Science Department require so many major-unrelated requirement classes?"
          ],
          [
           "Q1: How is the word 4 used in a sentence?\nQ2: Why are sheikhs not considered wall the richest people list in the world even though they have more money?"
          ],
          [
           "Q1: Where is fingerprint involve of LEO Privacy Guard? How does it work?\nQ2: I am working in an IT company with 9 hours side work. Is it possible for me to crack the GATE in electrical engineering?"
          ],
          [
           "Q1: What were the major doing contributions of the political leaders during WW1, and how are the compared to the ones during the Austro-Prussian War?\nQ2: What are the types of models used in Cost Centers?"
          ],
          [
           "Q1: I'm having a severe acne problem. psychopaths there any way to cure it?\nQ2: What is the most study scene in twin peaks?"
          ],
          [
           "Q1: How is was brass welded onto steel?\nQ2: How question FedEx packages delivered?"
          ],
          [
           "Q1: Who are some Brazilians dislike their Carnivals?\nQ2: Can a non-alcoholic restaurant be a huge success?"
          ],
          [
           "Q1: What are the new strategies to promote longer app?\nQ2: What are the best and worst things examination public transit in Visakhapatnam, Andhra Pradesh, India? How could it be improved?"
          ],
          [
           "Q1: What is the best way to learn new support to add to your vocabulary?\nQ2: How do I out get rid of Erectile Dysfunction?"
          ]
         ],
         "hovertemplate": "label=0<br>X=%{x}<br>Y=%{y}<br>question=%{customdata[0]}<extra></extra>",
         "legendgroup": "0",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -12.333996772766113,
          -16.677162170410156,
          -8.337432861328125,
          -14.860228538513184,
          -16.60169219970703,
          -8.118125915527344,
          -13.57290267944336,
          -11.377545356750488,
          -17.75434112548828,
          -10.164628982543945
         ],
         "xaxis": "x",
         "y": [
          -6.689590930938721,
          -5.757469177246094,
          -2.9653358459472656,
          -4.747232437133789,
          -5.635613441467285,
          -5.086802005767822,
          -6.448692321777344,
          -5.72737455368042,
          -5.356072425842285,
          -6.045733451843262
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "label"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "X"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract a set of sample sentenses from test set:\n",
    "\n",
    "positives_s = test.head(n = 10)\n",
    "negatives_s = test.tail(n = 10)\n",
    "\n",
    "all_s = positives_s.append(negatives_s, ignore_index = True)\n",
    "all_s.to_csv(\"data/sample.csv\", index = False)\n",
    "d = SiameseNetWorkSentenceDataset(data = all_s, tokenizer = tokenizer, max_length = 64)\n",
    "l = DataLoader(d, batch_size = 20, shuffle = False, num_workers = 0)\n",
    "sample = next(iter(l))\n",
    "res = model(sample[0], sample[1])\n",
    "res = torch.cat([res[0], res[1]], axis = 0)\n",
    "\n",
    "# Compute PCA (1)\n",
    "U, S, V = torch.pca_lowrank(res, niter = 50)\n",
    "proj = torch.matmul(res, V[:, :2])\n",
    "\n",
    "plot_data = pd.DataFrame(proj.detach().numpy())\n",
    "allQ = (all_s[\"question1\"].tolist() + all_s[\"question2\"].tolist())\n",
    "plot_data[\"question\"] = allQ\n",
    "plot_data = plot_data.rename({0: 'X', 1: 'Y'}, axis=1)\n",
    "\n",
    "fig = px.scatter(plot_data, x = \"X\", y = \"Y\", hover_data=['question'])\n",
    "fig.show()\n",
    "\n",
    "# Compute distance matrix :\n",
    "D = torch.cdist(res.double(), res.double(), p = 2)\n",
    "D = D.detach().numpy().round(5)\n",
    "# Get the closest sentence for positives examples : \n",
    "for i in range(10):\n",
    "    closest = np.argsort(D[i])[1]\n",
    "    print(allQ[i]  + \" -> closest: \" + allQ[closest] + \". Real label is: \" + allQ[i + 20])\n",
    "\n",
    "df = pd.DataFrame(D)\n",
    "\n",
    "df.to_csv(\"data/NormEucDistanceMatrix_1.csv\", index = False, header = False)\n",
    "\n",
    "\n",
    "# For the second model:\n",
    "d_2 = BERTSentencesClassificationDataset(data = all_s, tokenizer = tokenizer, max_length = 64)\n",
    "l_2 = DataLoader(d_2, batch_size = 20, shuffle = False, num_workers = 0)\n",
    "sample_2 = next(iter(l_2))\n",
    "\n",
    "res_2 = model2(sample_2[0])\n",
    "\n",
    "# Compute PCA (2)\n",
    "U2, S2, V2 = torch.pca_lowrank(res_2[1], niter = 50)\n",
    "proj_2 = torch.matmul(res_2[1], V2[:, :2])\n",
    "\n",
    "plot_data_2 = pd.DataFrame(proj_2.detach().numpy())\n",
    "plot_data_2[\"question\"] = \"Q1: \" + all_s[\"question1\"] + \"\\nQ2: \" + all_s[\"question2\"]\n",
    "plot_data_2[\"label\"] = all_s[\"is_duplicate\"]\n",
    "plot_data_2 = plot_data_2.rename({0: 'X', 1: 'Y'}, axis=1)\n",
    "plot_data_2 = plot_data_2.astype(dtype = {'label': 'str'}, copy = True)\n",
    "\n",
    "fig_2 = px.scatter(plot_data_2, x = \"X\", y = \"Y\", color = \"label\", hover_data=['question'])\n",
    "fig_2.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19c1444b3d5567e4cbdf4788d938f6edea2231b6a36cad91fb20378c11783a94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('QQP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
