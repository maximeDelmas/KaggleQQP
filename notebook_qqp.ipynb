{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import statistics\n",
    "from transformers import BertTokenizer, BertForPreTraining, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilitary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataset(dataset):\n",
    "    \n",
    "    # Check is all questions in 'question1' and 'question2' are str\n",
    "    filter = np.array([not isinstance(s1, str) for s1 in dataset['question1'].tolist()]) | np.array([not isinstance(s2, str) for s2 in dataset['question2'].tolist()])\n",
    "    indexes_to_drop = dataset[filter].index\n",
    "    \n",
    "    # drop lines that are not\n",
    "    if not len(indexes_to_drop):\n",
    "        print(\"All rows are corrects\")\n",
    "    else:\n",
    "        print(\"Removing the following lines: \")\n",
    "        print(dataset.loc[indexes_to_drop])\n",
    "        dataset = dataset.drop(indexes_to_drop)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the following lines: \n",
      "          qid1    qid2                         question1  \\\n",
      "id                                                         \n",
      "105780  174363  174364    How can I develop android app?   \n",
      "201841  303951  174364  How can I create an Android app?   \n",
      "363362  493340  493341                               NaN   \n",
      "\n",
      "                                                question2  is_duplicate  \n",
      "id                                                                       \n",
      "105780                                                NaN             0  \n",
      "201841                                                NaN             0  \n",
      "363362  My Chinese name is Haichao Yu. What English na...             0  \n",
      "Removing the following lines: \n",
      "                                           question1  \\\n",
      "test_id                                                \n",
      "379205      How I can learn android app development?   \n",
      "817520   How real can learn android app development?   \n",
      "943911                          How app development?   \n",
      "1046690                                          NaN   \n",
      "1270024             How I can learn app development?   \n",
      "1461432                                          NaN   \n",
      "\n",
      "                                               question2  \n",
      "test_id                                                   \n",
      "379205                                               NaN  \n",
      "817520                                               NaN  \n",
      "943911                                               NaN  \n",
      "1046690    How I what can learn android app development?  \n",
      "1270024                                              NaN  \n",
      "1461432  How distinct can learn android app development?  \n"
     ]
    }
   ],
   "source": [
    "# Paths & Variables\n",
    "\n",
    "data_path = \"data/quora-question-pairs\"\n",
    "train_file = \"train.csv\"\n",
    "test_pos_file = \"test.csv\"\n",
    "label_file = \"sample_submission.csv\"\n",
    "\n",
    "# Reading\n",
    "train = pd.read_csv(os.path.join(data_path, train_file), index_col = 0)\n",
    "test_pos = pd.read_csv(os.path.join(data_path, test_pos_file), index_col = 0)\n",
    "y_label = pd.read_csv(os.path.join(data_path, label_file), index_col = 0)\n",
    "\n",
    "# Fix datasets for NaN values in question1 or question2\n",
    "train = fix_dataset(train)\n",
    "test_pos = fix_dataset(test_pos)\n",
    "\n",
    "# join test and y_label\n",
    "test_pos = test_pos.join(y_label, on = 'test_id', how = 'left')\n",
    "\n",
    "# test set contains only positive, labels; suffle to create negative examples\n",
    "test_neg = test_pos.copy()\n",
    "test_neg['question1'] = np.random.permutation(test_neg['question1'])\n",
    "test_neg['is_duplicate'] = 0\n",
    "\n",
    "# Create final test set\n",
    "test = pd.concat([test_pos, test_neg], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggest that we could used bucketing and also filtering to a max length before initialyzing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "\n",
    "See (https://paperswithcode.com/method/wordpiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id for [CLS]: 101\n",
      "Token id for [SEP]: 102\n",
      "Token id for [PAD]: 0\n",
      "Token id for [UNK]: 100\n",
      "Token id for [MASK]: 103\n",
      "Original sentense: What is the step by step guide to invest in share market in india?\n",
      "Encoded sentense: \n",
      "[101, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1999, 2634, 1029, 102]\n",
      "Decoded sentense: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 13:26:10.145374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:\n",
      "2022-01-14 13:26:10.145390: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] what is the step by step guide to invest in share market in india? [SEP]\n",
      "La taille maximale de tokens est 286 (avec les [CLS] et [SEP])\n",
      "Il y a 99.88% des phrases tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Token id for [CLS]: \" + str(tokenizer.cls_token_id))\n",
    "print(\"Token id for [SEP]: \" + str(tokenizer.sep_token_id))\n",
    "print(\"Token id for [PAD]: \" + str(tokenizer.pad_token_id))\n",
    "print(\"Token id for [UNK]: \" + str(tokenizer.unk_token_id))\n",
    "print(\"Token id for [MASK]: \" + str(tokenizer.mask_token_id))\n",
    "\n",
    "print(\"Original sentense: \" + train.loc[0, 'question1'])\n",
    "print(\"Encoded sentense: \")\n",
    "enc = tokenizer.encode(train.loc[0, 'question1'])\n",
    "print(enc)\n",
    "print(\"Decoded sentense: \")\n",
    "dec = tokenizer.decode(enc)\n",
    "print(dec)\n",
    "\n",
    "# Check len of tokenized training sentences:\n",
    "list_len = []\n",
    "all_s = train['question1'].tolist() + train['question2'].tolist()\n",
    "for s in all_s:\n",
    "    tks = tokenizer.encode(s)\n",
    "    list_len.append(len(tks))\n",
    "\n",
    "max_len = max(list_len)\n",
    "\n",
    "print(f\"La taille maximale de tokens est {max_len} (avec les [CLS] et [SEP])\")\n",
    "lw_64 = round((sum([l <= 64 for l in list_len])/len(list_len)) * 100, 2) \n",
    "print(f\"Il y a {lw_64}% des phrases tokenised qui sont <= 64. C'est suffisant, on supprimera celle plus grande du dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "See https://pytorch.org/tutorials/beginner/basics/data_tutorial.html for documentation about the Dataset and Dataloader creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetWorkSentenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SiameseNetWorkSentenceDataset create a Dataset\n",
    "    - data (pd.DataFrame): the data dataframe with column 'question1' and 'question2' along with the label 'is_duplicate'\n",
    "    - tokenizer: the BERT tokenizer, such as: BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    - max_length: the maximal length of tokens input vector (default 64) Shorter vector arre padded to max_length with [PAD token] (id: 0) and longer are truncated. \n",
    "    The size includes the start [CLS] and end [SEP] tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        def squeeze_tensors(tks):\n",
    "            \"\"\"Take a tensor and remove unnecessary dimension. When using tokenizer with return_tensors = 'pt', the returned tensor is by default 2 dimensions, has it could handle a list of sentence as inputs.\n",
    "            However, as we only sent one sentence at a time to the tokenizer to create the Dataset, it result in an additional dimension that will be useless after pooling results by batches in the DataLoader\n",
    "\n",
    "            Args:\n",
    "                tks ([type]): [description]\n",
    "            \"\"\"\n",
    "            tks.data[\"input_ids\"] = torch.squeeze(tks.data[\"input_ids\"])\n",
    "            tks.data[\"token_type_ids\"] = torch.squeeze(tks.data[\"token_type_ids\"])\n",
    "            tks.data[\"attention_mask\"] = torch.squeeze(tks.data[\"attention_mask\"])\n",
    "\n",
    "        s1 = self.data.loc[index, 'question1']\n",
    "        s2 = self.data.loc[index, 'question2']\n",
    "        label = torch.tensor(self.data.loc[index, 'is_duplicate'])\n",
    "\n",
    "        tokens1 = self.tokenizer(text = s1, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "        squeeze_tensors(tokens1)\n",
    "        tokens2 = self.tokenizer(text = s2, max_length = self.max_length, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "        squeeze_tensors(tokens2)\n",
    "\n",
    "        return tokens1, tokens2, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([  101,  4118,  2000,  2424,  8745,  1997, 29199,  2478, 10424,  2229,\n",
      "        11877, 12170, 18098,  2964,  1029,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, {'input_ids': tensor([  101,  2054,  2024,  2070,  1997,  1996,  2477, 20202,  2064,  2425,\n",
      "         2055,  1996,  4241,  2527,  8553,  1998, 15258,  1997, 12191,  2015,\n",
      "         1998,  2049,  6177,  1029,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}, tensor(0))\n",
      "404287\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = SiameseNetWorkSentenceDataset(data = train, tokenizer = tokenizer, max_length = 64)\n",
    "print(train_dataset[10])\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size = 8, shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBERTNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseBERTNet, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.HS = self.bert.config.hidden_size\n",
    "\n",
    "    def forward_siamese(self, input):\n",
    "        \"\"\"From tokenised input sentence, compute BERT \n",
    "\n",
    "        Args:\n",
    "            input (dict): output dict from the tokenizer with input_ids, token_type_ids and attention_mask\n",
    "\n",
    "        Returns:\n",
    "            avg (tensor): Mean of the last hidden layer vectors for real tokens (attention_mask: 1) in the input.\n",
    "        \"\"\"\n",
    "        # Get input_ids and attention mask\n",
    "        input_ids, token_type_ids, attention_mask = input.values()\n",
    "\n",
    "        # Apply BERT and extract last_hidden_state\n",
    "        out = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        last_hidden_state = out.last_hidden_state\n",
    "\n",
    "        # Apply mean pooling on real tokens\n",
    "\n",
    "        # Get mask at the same dimension as last_hidden_state\n",
    "        expanded_attention_mask = attention_mask.unsqueeze(-1)\n",
    "        expanded_attention_mask = expanded_attention_mask.expand(-1, -1, self.HS)\n",
    "\n",
    "        # Element wise mul between last_hidden_state and mask to then only consider real tokens in the sum\n",
    "        prod = torch.mul(last_hidden_state, expanded_attention_mask)\n",
    "\n",
    "        # Sum all token vectors\n",
    "        sum_by_tks = torch.sum(prod, dim = 1)\n",
    "\n",
    "        # Get normalisation factor to compute mean\n",
    "        norm = torch.sum(attention_mask, dim = -1).unsqueeze(-1)\n",
    "\n",
    "        # Comptue average\n",
    "        avg = torch.div(sum_by_tks, norm)\n",
    "\n",
    "        return avg\n",
    "\n",
    "\n",
    "    def forward(self, question1, question2):\n",
    "        out1 = self.forward_siamese(question1)\n",
    "        out2 = self.forward_siamese(question2)\n",
    "\n",
    "        return out1, out2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SiameseBERTNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2421, -0.0127,  0.5213,  ..., -0.3123,  0.2232, -0.0228],\n",
       "         [-0.1228, -0.2455, -0.0334,  ..., -0.4755, -0.0187, -0.2791],\n",
       "         [ 0.3482, -0.0578,  0.0503,  ..., -0.5083, -0.0063,  0.0549],\n",
       "         ...,\n",
       "         [ 0.2691,  0.1998,  0.2084,  ...,  0.0679,  0.2034,  0.1144],\n",
       "         [ 0.2981,  0.0604,  0.0569,  ..., -0.4942, -0.1039,  0.3898],\n",
       "         [ 0.4200, -0.1842, -0.1692,  ..., -0.2427, -0.0343, -0.2598]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([[ 0.0481, -0.1291,  0.5596,  ..., -0.3893,  0.1819, -0.1425],\n",
       "         [ 0.3130, -0.1255, -0.0395,  ..., -0.2151,  0.0585, -0.2049],\n",
       "         [ 0.1844, -0.2187, -0.0291,  ..., -0.2617,  0.2435,  0.2495],\n",
       "         ...,\n",
       "         [ 0.2670, -0.0086, -0.0130,  ..., -0.2330,  0.3754,  0.1053],\n",
       "         [ 0.1539,  0.1170,  0.1969,  ..., -0.6343,  0.0204,  0.4126],\n",
       "         [ 0.4940, -0.3132, -0.0233,  ..., -0.2242,  0.1257, -0.4804]],\n",
       "        grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = next(iter(dataloader))\n",
    "model(test[0], test[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19c1444b3d5567e4cbdf4788d938f6edea2231b6a36cad91fb20378c11783a94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('QQP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
