{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 18:26:30.133247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-21 18:26:30.133264: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, 'app')\n",
    "\n",
    "from utils import set_seed, init_model\n",
    "from siamese_bert import ContrastiveLoss, SiameseNetWorkSentenceDataset, train_loop_siamese, SiameseBERTNet, pwdist_siamese_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIAMESEBERT /  BERTCLASSIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1024)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "OUT_DIR = os.path.join(\"data/test/SiameseBERT-no-train\")\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BATCH_SIZE = 128\n",
    "MODEL_PARAMS = dict({'freeze_embedding': True, 'freeze_encoder_layer': 8, 'freeze_cls_pooler': True})\n",
    "\n",
    "TEST = pd.read_csv(\"data/test/test_set.csv\", index_col=False)\n",
    "DATATEST = SiameseNetWorkSentenceDataset(data=TEST, tokenizer=TOKENIZER, max_length=64)\n",
    "TEST_LOADER = DataLoader(DATATEST, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Compute predictions for best-model on validation set\")\n",
    "MODEL = SiameseBERTNet(**MODEL_PARAMS)\n",
    "MODEL.load_state_dict(torch.load(\"data/SiameseBERT/batch128_m10_training/best-model.pt\"))\n",
    "MODEL.to(DEVICE)\n",
    "pwdist_siamese_bert(MODEL, TEST_LOADER, DEVICE, OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_cls_classif import BERTSentencesClassificationDataset, BERTSentencesClassification, bert_sentences_classification_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1024)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "OUT_DIR = os.path.join(\"data/test/BERT-classif\")\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BATCH_SIZE = 128\n",
    "MODEL_PARAMS = dict({'freeze_embedding': True, 'freeze_encoder_layer': 8, 'freeze_cls_pooler': True})\n",
    "\n",
    "TEST = pd.read_csv(\"data/test/test_set.csv\", index_col=False)\n",
    "DATATEST = BERTSentencesClassificationDataset(data=TEST, tokenizer=TOKENIZER, max_length=64)\n",
    "TEST_LOADER = DataLoader(DATATEST, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Compute predictions for best-model on validation set\")\n",
    "MODEL = BERTSentencesClassification(**MODEL_PARAMS)\n",
    "MODEL.load_state_dict(torch.load(\"data/BERTSentenseClassification/batch128/best-model.pt\"))\n",
    "MODEL.to(DEVICE)\n",
    "bert_sentences_classification_prediction(MODEL, TEST_LOADER, DEVICE, OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Siamese BERT result for PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siamese_bert import SiameseNetWorkSentenceDataset, SiameseBERTNet\n",
    "\n",
    "\n",
    "def test_pca(model, test, device, out_dir, tokenizer):\n",
    "    \n",
    "    out_dim = model.hidden_size\n",
    "\n",
    "    # set the model to eval:\n",
    "    model.eval()\n",
    "\n",
    "    emb_q1 = np.array([]).reshape(0, out_dim)\n",
    "    emb_q2 = np.array([]).reshape(0, out_dim)\n",
    "    y_list = []\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    # Iterate over testing batchs\n",
    "    for step, batch in enumerate(test):\n",
    "\n",
    "        # Get batch data\n",
    "        input_ids_q1 = batch[0]['input_ids'].to(device)\n",
    "        attention_mask_q1 = batch[0]['attention_mask'].to(device)\n",
    "        input_ids_q2 = batch[1]['input_ids'].to(device)\n",
    "        attention_mask_q2 = batch[1]['attention_mask'].to(device)\n",
    "        y = batch[2].to(device)\n",
    "\n",
    "        # Apply model\n",
    "        out_q1, out_q2 = model(input_ids_q1, attention_mask_q1, input_ids_q2, attention_mask_q2)\n",
    "        \n",
    "        s1 += [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in input_ids_q1]\n",
    "        s2 += [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in input_ids_q2]\n",
    "\n",
    "        emb_q1 = np.concatenate([emb_q1, out_q1.detach().numpy()])\n",
    "        emb_q2 = np.concatenate([emb_q2, out_q2.detach().numpy()])\n",
    "\n",
    "        y_list += y.tolist()\n",
    "\n",
    "    pd.DataFrame(emb_q1).to_csv(os.path.join(out_dir, \"emb_q1.csv\"), index=False, header=False)\n",
    "    pd.DataFrame(emb_q2).to_csv(os.path.join(out_dir, \"emb_q2.csv\"), index=False, header=False)\n",
    "    pd.DataFrame({\"y\": y_list}).to_csv(os.path.join(out_dir, \"y.csv\"), index=False, sep =',')\n",
    "    pd.DataFrame({\"s1\": s1, \"s2\": s2}).to_csv(os.path.join(out_dir, \"setences.csv\"), index=False, sep ='\\t')\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BATCH_SIZE=128\n",
    "\n",
    "test_set = pd.read_csv(\"data/test/test_set.csv\", index_col=False)\n",
    "pos_eg = test_set[test_set[\"is_duplicate\"] == 1].sample(frac=1).head(n=10)\n",
    "neg_eg = test_set[test_set[\"is_duplicate\"] == 0].sample(frac=1).head(n=10)\n",
    "new_test_set = pd.concat([pos_eg, neg_eg])\n",
    "new_test_set.reset_index(drop = True, inplace = True)\n",
    "\n",
    "DATATEST = SiameseNetWorkSentenceDataset(data=new_test_set, tokenizer=TOKENIZER, max_length=64)\n",
    "LOADER = DataLoader(DATATEST, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "MODEL = SiameseBERTNet()\n",
    "MODEL.to(DEVICE)\n",
    "test_pca(MODEL, LOADER, DEVICE, \"data/SiameseBERT/batch128_m10_training/pca-data/no-train\", TOKENIZER)\n",
    "\n",
    "\n",
    "MODEL = SiameseBERTNet()\n",
    "MODEL.load_state_dict(torch.load(\"data/SiameseBERT/batch128_m10_training/best-model.pt\"))\n",
    "MODEL.to(DEVICE)\n",
    "test_pca(MODEL, LOADER, DEVICE, \"data/SiameseBERT/batch128_m10_training/pca-data/train\", TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_output_vectors(model, test, device, out_dir, tokenizer):\n",
    "    \n",
    "    out_dim = model.hidden_size\n",
    "\n",
    "    # set the model to eval:\n",
    "    model.eval()\n",
    "\n",
    "    emb_q1 = np.array([]).reshape(0, out_dim)\n",
    "    emb_q2 = np.array([]).reshape(0, out_dim)\n",
    "    y_list = []\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    # Iterate over testing batchs\n",
    "    for step, batch in enumerate(test):\n",
    "\n",
    "        print(f\"step: {step +1}\")\n",
    "\n",
    "        # Get batch data\n",
    "        input_ids_q1 = batch[0]['input_ids'].to(device)\n",
    "        attention_mask_q1 = batch[0]['attention_mask'].to(device)\n",
    "        input_ids_q2 = batch[1]['input_ids'].to(device)\n",
    "        attention_mask_q2 = batch[1]['attention_mask'].to(device)\n",
    "        y = batch[2].to(device)\n",
    "\n",
    "        # Apply model\n",
    "        out_q1, out_q2 = model(input_ids_q1, attention_mask_q1, input_ids_q2, attention_mask_q2)\n",
    "        \n",
    "        s1 += [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in input_ids_q1]\n",
    "        s2 += [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in input_ids_q2]\n",
    "\n",
    "        emb_q1 = np.concatenate([emb_q1, out_q1.detach().numpy()])\n",
    "        emb_q2 = np.concatenate([emb_q2, out_q2.detach().numpy()])\n",
    "\n",
    "        y_list += y.tolist()\n",
    "    \n",
    "    all_questions_emb = np.concatenate([emb_q1, emb_q2])\n",
    "    all_sentences = s1 + s2\n",
    "\n",
    "    return all_questions_emb, all_sentences, y_list\n",
    "\n",
    "\n",
    "test_set = pd.read_csv(\"data/test/test_set.csv\", index_col=False)\n",
    "new_test_set = test_set.sample(frac=1).head(n=1000)\n",
    "new_test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BATCH_SIZE=128\n",
    "DATATEST = SiameseNetWorkSentenceDataset(data=new_test_set, tokenizer=TOKENIZER, max_length=64)\n",
    "LOADER = DataLoader(DATATEST, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "MODEL = SiameseBERTNet()\n",
    "MODEL.load_state_dict(torch.load(\"data/SiameseBERT/batch128_m10_training/best-model.pt\"))\n",
    "MODEL.to(DEVICE)\n",
    "emb_matrix, all_s, y = test_output_vectors(MODEL, LOADER, DEVICE, \"data/SiameseBERT/batch128_m10_training/pca-data/train\", TOKENIZER)\n",
    "\n",
    "pwdist = pairwise_distances(emb_matrix, metric='euclidean')\n",
    "\n",
    "out_dir = \"data/SiameseBERT/batch128_m10_training/neighbourhood_data/n1000\"\n",
    "pd.DataFrame(pwdist).to_csv(os.path.join(out_dir, \"emb_questions.csv\"), index=False, header=False)\n",
    "pd.DataFrame({\"question\": all_s}).to_csv(os.path.join(out_dir, \"sentences.csv\"), index=False, sep ='\\t')\n",
    "pd.DataFrame({\"y\": y}).to_csv(os.path.join(out_dir, \"y.csv\"), index=False, sep =',')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19c1444b3d5567e4cbdf4788d938f6edea2231b6a36cad91fb20378c11783a94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('QQP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
